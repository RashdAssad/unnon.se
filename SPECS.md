AI Website Replicator – Specifications (SPECS.md)
Vision and Scope
The AI-Assisted Website Replicator aims to let users clone the structure and design of existing websites with minimal effort. Given a target URL, the system will analyze the site’s pages and components, then generate a new project that closely mirrors the original (approximately 85–90% structural accuracy up to 3 levels deep in navigation). Users can customize the clone with their own branding (e.g. company name, logo, images) and minor style tweaks. The focus of v1 is on speed and practicality: producing a maintainable Next.js codebase that captures the target site’s layout and styling, rather than a pixel-perfect copy. The product will empower developers and designers to jumpstart new web projects by leveraging the layouts of existing sites, accelerating development while keeping the human user in control of final refinements[1][2].
Key Features and Use Cases
    • Clone Static Websites from a URL: Users can input the URL of a mostly static website. The system will fetch the pages (up to depth 3 from the homepage) and reconstruct their structure and styling in a new Next.js project. Interactive elements (navigation, basic forms) will be replicated to the extent possible, while complex server-side logic or database content is out of scope for v1.
    • Brand Substitution: After cloning, users can provide a new site title, branding info, and up to 5 image files (e.g. a logo and other media). The tool replaces the original site’s logos and related images with the uploaded ones, and swaps out texts like company name or product names with the user’s provided values. This allows quick rebranding of the replicated site.
    • Basic Style Adjustments: The replicator will detect the source site’s design tokens (colors, fonts, etc.) and allow simple adjustments. Users can input high-level instructions or select options (for example: “Change primary color to teal” or “Use a playful font”). The system will then tweak the generated Tailwind CSS configuration or component styles accordingly.
    • Downloadable Code Package: The output is a downloadable ZIP containing the full source code of the generated site. This includes a Next.js project with pages and components, a Tailwind CSS setup (and component library integration), and all assets (including the user’s uploaded images). The code will be organized and commented for easy follow-up editing. Optionally, the user can deploy the site directly to Vercel or another static host after generation.
    • AI-Assisted Code Generation: Under the hood, the system uses AI agents to analyze the target site and generate code. The AI is guided by prompt templates and the project’s specified tech stack to output clean React/TypeScript components (using a consistent design library). The agent aims for maintainable code – for example, extracting repeated page elements into reusable components. It follows best practices (semantic HTML, DRY code structure) and adheres to the chosen styling system, with comments indicating any uncertainties that might need human review.
    • Optional Guided Refinement: For advanced use, the system can output intermediate specification and plan files (in the style of Gemini Conductor, e.g. specs.md and plan.md) describing the site structure and tasks done. This allows a developer to review the AI’s understanding of the site and make manual adjustments if needed. At key stages (like after planning, or before applying styles), the user might be prompted to confirm or tweak the AI’s approach – ensuring a human-in-the-loop for critical design decisions.
Tech Stack
Frontend: The front-end is built with Next.js (TypeScript), deployed on Vercel for its excellent support of serverless functions and static asset serving. Next.js will provide the user interface where one can input the target URL, upload branding files, and initiate the replication process. It also will present results (e.g. a preview or success page with the download link). To monitor and maintain a snappy UI, we leverage Next.js performance features (automatic code-splitting, Web Vitals tracking) and Vercel’s analytics. The UI will be minimal in v1 – a form for input and a progress display – but it will load quickly and be mobile-responsive.
Backend: The heavy-lifting occurs in a Python service responsible for web scraping and AI coordination. This Python backend may be deployed as a separate service (e.g. a Cloud Run container or a serverless function) that the Next.js app calls via REST API. Python is chosen due to its rich ecosystem for scraping (Playwright) and compatibility with many AI libraries and APIs. It will orchestrate multi-step AI interactions and perform any necessary post-processing on the generated code. Communication between the Next.js front-end and Python back-end uses JSON serialization over HTTPS (for example, a POST request containing the target URL and user customizations). This hybrid architecture ensures we can use the best tools in each domain: the React/TS front-end for interactivity and the Python back-end for AI and data processing.
AI and Agents: We plan to integrate Google’s Gemini generative models via API (or an SDK) for the AI tasks. The AI agent(s) will be invoked from the Python backend. We’ll use models suitable for code generation and reasoning, such as Gemini 2.5 or Gemini 3 (Flash or Pro depending on the needed context length vs speed). The system may also use OpenAI’s models or others as fallbacks if needed, but the design is optimized for Gemini’s advanced capabilities in multi-step coding tasks[3][4]. For multi-agent orchestration, we are influenced by Google’s Agent Development Kit (ADK) and the Gemini CLI’s Conductor extension, though in v1 the orchestration logic is custom Python. In the future, parts of the agent logic could be implemented in TypeScript using the ADK (to unify the stack), but initially Python will coordinate a Planner agent and Coder agent as described in the Architecture.
Libraries & Frameworks: Key libraries include Playwright (headless browser) for DOM extraction, BeautifulSoup or lxml for parsing HTML when needed, and Tailwind CSS (with a component kit like shadcn/ui) for consistent UI components in the generated code. We specifically plan to use shadcn/ui (a Radix/Tailwind-based component library) to speed up styling and ensure consistency. According to an analysis of same.new (a similar tool), using Next.js with Tailwind and shadcn/ui allows quick assembly of common UI elements with consistent styling[5]. This means the AI doesn’t need to generate all CSS from scratch – it can apply Tailwind utility classes and compose pre-built components, drastically simplifying the code generation problem. The backend will also use the Google Generative AI SDK (google-generativeai for Python[6]) or the REST API to interface with Gemini models. For handling uploads, we may use a small file storage service (like an S3 bucket or Vercel Blob storage) since Vercel serverless functions have limited runtime storage.
Architecture Overview
Overall Flow: The user interacts with the Next.js frontend to provide input. When a replication is initiated, the frontend sends the details (target URL and any user preferences/branding data) to the Python backend via a secure API call. The backend then orchestrates the following steps: 1. Scraping & Analysis: The Python service launches a Playwright browser to load the target URL (this ensures dynamic content is fully rendered). It captures the final DOM and possibly a full-page screenshot. Using a combination of DOM tree analysis and visual segmentation, the backend identifies distinct sections/components on the page[7]. This could involve analyzing HTML structure (e.g. looking for headers, footers, nav menus, content sections based on tags and styles) and using heuristic or ML-based vision models to segment the screenshot into regions (for example, detecting separate content blocks by layout). The result is an intermediate representation of the page’s structure – essentially an Abstract Syntax Tree (AST) that lists pages, sections, and components with their content and styles. If the site has multiple pages (links), the scraper will navigate up to 3 levels deep to gather those pages’ structures as well, building a site map. 2. Planning with AI (Project Spec): The backend invokes the Planner Agent (an AI prompt) with the gathered site structure. The planner’s job is to turn the raw analysis into a project specification – identifying what pages and components need to be created in the new project and how they relate. It will formalize requirements such as: “Page A has a navbar with logo and menu, a hero section with background image X and heading Y, a features section with a 3-column layout, and a footer… Page B is a contact form page,” etc. The planner agent essentially produces a specs.md style summary of the site’s content and a plan.md (an ordered list of tasks or components to build)[8][9]. (In the automated pipeline, these may remain in-memory structures, but for transparency we can output them as Markdown for developers to inspect.) This planning step ensures the subsequent code generation is guided by a clear breakdown, aligning with the “measure twice, cut once” philosophy[1] of Conductor. 3. Component Code Generation: Next, the Coder Agent takes each item from the plan (e.g., “Build the header component with logo and navigation links”) and generates the actual source code. We instruct the agent with a system message containing the tech stack details (e.g. “Use Next.js with React, functional components, Tailwind CSS for styling, and components from shadcn/ui where applicable”) so that the output follows our stack choices. The agent likely uses a hybrid approach to code generation: for well-known structures it can rely on templates (we will maintain a set of snippet templates for common components like navbars, footers, cards) and fill in specifics, while for unique or complex sections it can reason and produce code using the LLM[10]. This approach was observed in same.new, which likely combines template engines and an AI model for intricate parts[10]. The coder agent works through the list: generating a Next.js page for each HTML page, React components for each distinct UI section, applying Tailwind classes to approximate the original styles, and ensuring links and interactions (like modals or toggles) are included if present. The code for each component is written to the project structure (the Python backend will actually create the files on disk or in memory). 4. Styling and Asset Integration: After base code generation, a styling pass ensures that the Tailwind theme matches the original site’s color palette and font choices. For example, if the source site’s primary color was a certain hex value, the system will add that to the Tailwind config as the primary color and use it in relevant classes (or apply classes like text-[#hex] if needed)[11]. The AI agent will have initially guessed at Tailwind classes; a post-process may adjust those if the screenshot analysis reveals exact color codes or spacing values that differ. All user-provided assets are also integrated at this stage: e.g., the default logo image path in the code is replaced with the user’s uploaded logo.png (and that file is added to the project’s public/assets folder). We limit to 5 uploads in v1, so if the original site had more images, we only replace the main ones (others will still link to the original or use placeholders). Any textual brand replacements (name, slogans) are performed across the generated content – either by instructing the AI during generation or by a find-and-replace on the output files. 5. Packaging & Delivery: Once code generation and branding substitution are done, the backend assembles the project into a ZIP file. This includes all source files, a package.json (with dependencies like Next, React, Tailwind, shadcn/ui), configuration files for Tailwind and possibly a deploy config (same.new, for instance, uses a Netlify config[12]; in our case, we might include a minimal Vercel config or simply rely on Next defaults). The backend returns this ZIP to the frontend, which triggers a download for the user. We also show the user a summary of what was generated (e.g., number of pages, components, and any notes if some parts couldn’t be perfectly cloned). In a future iteration, we could offer one-click deploy to Vercel, but for v1 the user will manually import the project if they want to host it.
Throughout this pipeline, performance and reliability are considerations. The scraping and AI steps can be time-consuming, so we’ll design the API to handle it asynchronously: when a replication job starts, the frontend will poll for status updates (or receive them via web socket/events) so the user sees progress (e.g., “Scraping page 2 of 5… Generating code for header… Done.”). If any step fails (e.g., target site not reachable or AI error), the system will time out gracefully and report the error to the user with suggestions (perhaps asking to try a simpler page or check the URL).
Functional Requirements
Core Functionality
    • Website Fetching & DOM Analysis: The system must fetch the target website’s HTML and assets, executing any client-side JavaScript to capture the fully rendered page. This should be done in a way that mimics a real browser (to handle SPAs or sites that load data dynamically). Using Playwright in headless mode, the backend will load the page and wait for network idle. Then it will extract the DOM structure and important styling information. The analysis should identify distinct sections of the page. Acceptance criteria: Given a known static site, the scraper outputs a structured representation that lists all major elements (e.g., detects header, content sections, footer) and captures text, image sources, and key style attributes[13][14].
    • Multi-page Crawl (Depth-3): The tool will follow internal links up to 3 clicks away from the start page. For each discovered page, it repeats the DOM extraction. We will limit this to avoid crawling entire large sites – focusing on primary navigation links and perhaps one additional level for each. Acceptance criteria: If the user inputs a site with a homepage, about page, and blog section with posts, the replicator should fetch the homepage, the about page, and some blog post pages up to depth 3. It should not get stuck in infinite crawl or go beyond the domain.
    • Component AST Representation: Internally, the system should convert raw HTML/DOM into an AST or similar intermediate format. This structure will list components hierarchically (e.g., Page -> Section -> sub-components). Each node in the AST contains content (text/images) and style info (classes, inline styles, bounding box from screenshot segmentation if available). This abstraction is crucial for prompting the AI in a structured way, and for possibly reusing components across pages. Acceptance criteria: The AST can be serialized to JSON for inspection, showing nested components with their content. It should abstract away page-specific details and focus on reusable patterns (for example, a recurring testimonial card design is represented once and reused N times).
    • Code Generation (Next.js & Tailwind): The system will generate a Next.js project including pages and components that mirror the AST. For each page, a .tsx file (or .jsx with TypeScript types) is created. Shared layout pieces (navbars, footers) are generated as separate components and imported where needed. The code should use functional React components with hooks if needed (e.g., for interactive menus). Styling is applied via Tailwind CSS classes in JSX, and global styles or custom CSS should be minimized in favor of Tailwind utilities. We will integrate shadcn/ui components for common UI elements (buttons, modals, forms) to avoid reinventing the wheel – the AI can be instructed to use those (for example, use <Button> from shadcn rather than a custom styled <button>). Acceptance criteria: The generated project runs with npm install && npm run dev and visually resembles the original site. Key elements are present and styled similarly. The code adheres to the structure (Tailwind classes, shadcn components) specified.
    • Asset and Content Replacement: The user’s provided content must be injected into the generated site. This includes static assets (images) and text (like site name, taglines). Implementation-wise, the system will either: a) instruct the AI during generation by providing these inputs (e.g., “Use this logo image for the header”), or b) post-process the code by replacing occurrences of the original content. Likely a combination is used: we know the main logo image from the analysis (e.g., the <img> in the header), so we directly substitute that with the user’s logo. For texts, we can do find-replace for the original site name. Acceptance criteria: In the final output, the user’s branding is present – their logo file is referenced in the code, their site name appears in headings, and none of the original names/logos remain.
    • Customization via Prompts: The system should accept simple user instructions for style modifications. For v1, this can be a text prompt input (optional) where a user can say one or two things like “make the buttons rounded” or “use a dark theme”. The Planner or Coder agent will take this into account (likely by appending to the system or user prompt context). Acceptance criteria: If a user provides a style prompt, the generated site reflects that change (within reason). For example, asking for a “dark theme” results in a darker color scheme than the original (perhaps by toggling Tailwind’s dark mode or adjusting colors).
    • Export & Deployment: The finished project is packaged as a ZIP for download. The system must ensure the archive includes all necessary files to run the site. In addition, the front-end will offer a quick preview if possible (e.g., render the home page of the new site in an iframe or as a static HTML preview) so the user can verify success without leaving. Deployment for v1 is manual (the user can take the code and deploy it to Vercel or Netlify). We will include documentation or notes in the README about how to deploy. Acceptance criteria: The user can download the ZIP, open it, and see a structured project with a README, pages directory, etc. After running the project locally or deploying, it should closely resemble the target site in structure.
Integration and AI Requirements
    • AI Model Integration: The system will integrate with the Gemini AI API (or CLI) to leverage LLM capabilities. It must handle authentication (API keys or OAuth for Google’s API) and abide by usage quotas. The Gemini 3 model offers a very large context window (~1 million tokens)[3] which is advantageous for feeding in entire pages of HTML or large spec documents. However, for cost and speed, we may use a smaller model (Gemini “Flash” variant) for certain tasks. The design should allow choosing model variants or switching to an alternative (like GPT-4) via configuration. We’ll use REST endpoints or official SDKs in the Python backend to send prompt data and receive generated content. Acceptance criteria: The AI calls succeed within the expected time, and the responses (code or text) are incorporated without errors. We handle errors by catching API exceptions or timeouts and providing fallback responses (like “The AI could not generate this component. Please fill it in manually.” as a comment in code).
    • Gemini CLI & Conductor Integration (Optional): For developers using this tool, we integrate with the Gemini CLI Conductor workflow. This means the replicator can produce the standard Markdown files (specs, plans, and todos) and even a setup.md capturing project context. If the user/developer has the Gemini CLI, they could drop these files into their repo and run /conductor:implement to continue working on the project with AI assistance[15][16]. The backend should follow Conductor’s conventions for content in these files (like listing features in spec.md, tasks in plan.md). Acceptance criteria: If enabled, the output ZIP contains a conductor/ directory with product.md, tech-stack.md, etc., and a track folder with spec.md and plan.md for the generated site’s build. These should be populated with meaningful content (e.g., product goals reflecting the user’s project, and tasks that were executed to build the site). This feature won’t affect end-users who just want the site, but adds value for those treating the clone as a base for further development.
    • Multi-Agent Orchestration: The architecture should allow using multiple AI “agents” or roles collaborating. In v1, this is largely hidden in the backend logic (we sequentially prompt for plan then code). However, the system design recognizes roles like Planner and Coder, and could be extended to concurrent agents (e.g., generating multiple components in parallel threads, or using an agent to verify the code). We ensure that the agents share context properly – e.g., the Coder agent gets the spec produced by the Planner agent so it knows the plan. The serialization format between agents is important (using a structured plan ensures consistency). Acceptance criteria: The Planner’s output is coherent and comprehensive enough that the Coder doesn’t need to ask for clarification. If we were to run two agent prompts in parallel (say for two independent pages), the system can do so without conflicts (ensuring thread-safe use of any resources like file writes).
    • Performance Metrics & Limits: We impose some practical limits to keep the system performant in v1. This includes the aforementioned depth limit (3) and file upload cap (5 files, each perhaps max 5 MB). We also limit the maximum DOM size or number of pages to process (for instance, we might refuse to clone an entire documentation site with 50+ pages in one go). The system should warn the user if limits are exceeded (“The target site is too large to clone at once. Please try a smaller section.”). On the front-end, we track how long each major step takes, using that for both user feedback and performance monitoring. Vercel’s monitoring and Next.js custom metrics will log the end-to-end time and possibly resource usage. Our goal is for a typical single-page site clone to complete in under 2-3 minutes (assuming moderate network and model latency), and a multi-page (depth 3) clone to finish in under 5 minutes. Acceptance criteria: Under test conditions (e.g., cloning a known template site with ~3 pages), the process completes within the target time. The system remains responsive (progress bar updates) and does not exceed memory/compute quotas (e.g., avoid timeouts on Vercel lambdas by offloading heavy tasks to the separate Python service with sufficient timeout settings).
Non-Functional Requirements
    • Performance (Frontend): The Next.js frontend should be lightweight and fast. We will use Next’s optimized image and script handling for the UI. The form and result pages will load in milliseconds, and user interactions (starting a job) should feel responsive. We’ll utilize Next.js Web Vitals to track metrics like Time to Interactive and Largest Contentful Paint for our UI, ensuring that even on slower connections the app interface performs well. The front-end will also efficiently handle the download of the ZIP (streaming it if necessary to avoid memory issues). Any heavy computations are offloaded to the backend; the frontend just polls status and updates the UI accordingly.
    • Performance (Backend): The scraping and AI steps are the most time-consuming. We optimize by using concurrency where possible: for example, crawling pages depth-3 can be done in parallel up to a limit (maybe 2-3 pages at a time to not overload the target server or our resources). We also consider caching: if the same URL is requested again, we might reuse previous analysis or at least the static assets to avoid re-downloading (though caching is optional in v1). The Python backend should be sized appropriately (e.g., run on a VM with sufficient memory to load headless browser and handle the large context for the AI). We anticipate token usage can be high when feeding entire pages to the model[17], so we’ll keep prompts concise by summarizing or truncating less important parts of the HTML if needed. Through careful prompt design and possibly using the smaller “Flash” model for code tasks, we keep latency reasonable.
    • Scalability: In v1, the priority is shipping quickly, so the system might not be ready for heavy multi-user load. However, it should be designed to scale horizontally in the future. This means the backend statelessly handles one job at a time (per process), and we can run multiple instances behind a queue or load balancer to serve many concurrent requests. For now, we might implement a basic job queue (in memory or using a simple Cloud Task queue) to ensure if many requests come in, they are processed one by one to not overwhelm the AI API or scraping resources. The architecture using separate services (frontend on Vercel, backend on Cloud Run) already supports independent scaling of each component.
    • Reliability & Error Handling: The system should handle common failure modes gracefully. If the target URL is invalid or the site is unreachable, the user gets a clear error message (“Could not fetch the website. Please check the URL and try again.”). If the AI generation fails (e.g., model returns an error or nonsense), the system will catch it and either retry with a simpler prompt or mark that component for manual attention. We will implement timeouts for each step (e.g., scraping must finish in <30s per page, AI calls in <60s each) to avoid hanging. Logging is important for debugging in v1 since issues may arise; both frontend and backend will log key events and any exceptions (these logs can be routed to a monitoring console).
    • Maintainability: The generated codebase (output) should be easy for humans to understand and maintain. This is a quality of the functional output, but it’s also a non-functional goal: to treat the output as production-quality starter code, not just a throwaway prototype. Thus, our prompt templates and logic enforce consistent naming conventions and component organization. Internally, our own code (the replicator codebase) will be well-organized too – the repository will have clear separation of the Next.js app and the Python backend, with documentation (these markdown files) guiding new contributors. We will follow standard coding practices in both TypeScript and Python, and include comments especially in prompt definitions to explain the reasoning to future developers (per the requirement for well-commented prompt templates).
    • Security: Users will potentially input URLs and download code – we must ensure this process is secure. The backend should sanitize the target URL (only allow http/https, no file:// or internal addresses). The headless browser runs in a sandboxed environment to avoid executing any malicious code from the site on our server (Playwright default isolation should suffice, possibly run it with --disable-web-security only if needed for CORS but otherwise locked down). The file uploads (logos/images) will be scanned or at least validated as image types to prevent any malicious file injection. When integrating with the AI API, we’ll adhere to safety guidelines – e.g., using safety filters if provided by the API and not sending user-uploaded content that violates policies. Since this is a dev tool, we expect users to use it on allowed content, but we still include a disclaimer about respecting target site terms and copyrights.
    • Legal & Ethical Considerations: Cloning websites can raise copyright and ethical issues. The tool will include a disclaimer that the user must have rights or permission to replicate a site’s design/content, or use it only for learning purposes. We will not store any scraped content long-term on our servers beyond the processing needed. If a target site has a robots.txt or anti-scraping measures, the tool should respect those (for example, if explicitly disallowed, we either refuse or warn the user). This aligns with fair use and legal use guidelines[18]. We’ll also ensure that if the AI is used to generate content, it doesn’t knowingly include copyrighted material (the AI should produce its own code rather than copying verbatim large swaths of the original – except things like text content which the user might want; that is a grey area we’ll clarify in terms and potentially give an option to exclude original text beyond placeholders).
Integrations and External Services
    • Gemini AI API: As noted, the primary integration is with Google’s Gemini models. We’ll obtain API access (likely via Google Cloud Vertex AI or AI Studio keys). This integration will be configured via environment variables for the API key and model selection. We will incorporate best practices from Google’s guides – e.g., using the Gemini 3 Pro model for complex reasoning like site planning (for its higher reasoning depth), and possibly Gemini 3 Flash for faster code generation once the plan is set, leveraging adaptive model usage for speed[19][20]. The integration also includes using Google’s MCP (Model Context Protocol) if needed for tools (though in v1, we might not use advanced MCP tools beyond basic web fetch which we handle ourselves via scraping).
    • Gemini CLI (Conductor & Jules): For development, our team will use the Gemini CLI’s Conductor extension to manage this project’s development workflow. The product itself doesn’t require end-users to have Gemini CLI, but internally we treat Conductor as a blueprint. We may use Conductor’s output files to generate these documentation files (specs, plan, etc. for our own planning) and possibly to validate that our plan generation aligns with Conductor’s format. Additionally, the Jules extension (for asynchronous coding tasks) could be leveraged during development to offload writing repetitive code in the replicator. For example, while building the AST parser, a developer could delegate a function implementation to Jules via /jules command and have it coded in the background[21][22]. These integrations streamline our development process – they won’t run in production, but they influence how we structure agent prompts and how we think about multi-agent collaboration.
    • Headless Browser (Playwright): We integrate Playwright for web scraping. This is an external dependency managed via Python’s package. It requires browser binaries – on Vercel or Cloud Run we’ll use the headless Chromium provided or use Playwright’s ability to download a compatible binary at runtime. This integration may require some extra setup in the Docker environment (ensuring fonts and libraries for rendering are available).
    • Storage and File Hosting: For handling user-uploaded files and possibly large intermediate data, we integrate with a storage solution. In v1, since the file upload limit is small (5 images), we might not need a complex storage; the files can be kept in memory or ephemeral storage and included directly in the ZIP. If we decide to upload them (to avoid large payloads through our API), we could use a temporary blob storage (like AWS S3 or Cloud Storage) where the frontend uploads the file and passes a reference. This is an optional integration; for simplicity, initial implementation may just Base64-encode small images in JSON (not performance-friendly for big images, but acceptable for a few logo files).
    • Logging/Monitoring Services: To ensure smooth operation, we will integrate basic logging. This could mean using Vercel’s logging for frontend and Cloud Run’s logs for backend. If needed, we might add an error reporting service (like Sentry) especially in the backend to catch exceptions in scraping or AI calls. This will help in iterating quickly on v1 by catching issues early.
    • CI/CD and Version Control: The project will be in a Git repository. We might set up GitHub Actions or a similar CI to run tests (for instance, a small set of known sites to clone as regression tests). Also, deploying to Vercel is linked to the repo (Vercel auto-deploys on push to main). For the Python backend, we’ll create a Docker image and possibly integrate a GitHub Actions workflow to build and deploy it to Cloud Run. All these ensure that as we improve the code, deployment is quick – aligning with the speed-to-ship mindset (we prefer straightforward deployment pipelines over elaborate, at least for v1).
Branding and Customization
One of the selling points of the replicator is easy branding customization: - Users can upload a logo and up to 4 additional images. The UI will clearly show the number of remaining images they can add. We support common formats (PNG, JPG, SVG for logos if possible). The system will automatically optimize these images for web (e.g., resizing if the original is too large, maybe generating a WebP if needed for performance, though not mandatory in v1). These images will then replace the most prominent images on the site. Our analysis step will attempt to identify which images in the site are logos or key graphics (for instance, by size and location – usually the site’s header logo is identifiable). We’ll map user uploads to these identified images: e.g., the first upload is assumed to be the primary logo (with guidance text to the user to upload their logo first), the second could replace a hero/banner image, etc. Any user images that remain unused (if the site had fewer than 5 images to replace) will just be placed in an /assets folder for the user to use as they see fit. - Color Scheme and Typography: The replicator will extract the primary color palette of the source site. We allow the user to adjust the primary and secondary colors via color pickers or input codes before generation. If they don’t, we default to the original site’s colors (or we might apply a default palette if the user wants a full re-theme). Similarly, if the user wants a different font, we could allow selecting from a few web-safe fonts or Google Fonts. In v1, we might keep this simple or skip font customization (just use what’s in the original or a neutral default if not easily obtainable). Branding options might also include toggling dark/light mode (if user wants the clone in dark mode regardless of source). - Content Edits: Aside from global text like site name, we could let users provide a few replacement texts (like tagline or contact info). We’ll present fields for those if the analysis finds corresponding content. For instance, if the site has a tagline in the hero section, we show a field “Hero tagline” pre-filled with the original text, allowing the user to change it. This ensures that after generation, the site isn’t full of someone else’s content – it feels like their site. - Output Branding: We will allow the user to name the project (for instance, if they enter “MySite” as project name, we use that in the README and maybe as the app’s title). The zip filename could incorporate this name too. We also include any uploaded logos in an assets/ folder, and if it makes sense, set the Next.js <title> and metadata to the new site name. Essentially, we want the delivered package to be ready-to-use for the user’s brand with minimal further changes. - Limitations: We clearly communicate that in v1 not every branding aspect is automated. If the user wants more extensive changes (like “make the layout 2-column instead of 3” or “change all images to round”), those may be beyond the current capabilities except via manual code edits after generation or via prompt tinkering (which might not always work reliably). The focus is on straightforward branding changes – name, logo, color – which can be handled robustly.
Summary
In summary, the AI Website Replicator v1 is a tool for rapidly cloning website layouts and styling into a new, customizable codebase. It leverages a hybrid architecture (Next.js + Python) to use the right tools for each job, and integrates cutting-edge AI (Gemini models) to do the heavy lifting of code generation. The product prioritizes speed to first draft over perfect fidelity: it will get you most of the way to an MVP website that looks like the inspiration site, letting you then fine-tune the remaining 10–15%. All decisions in this specification (from using Tailwind and a component library, to limiting scope and focusing on brand swaps) are made to serve that goal. Future versions can iterate on this base, improving accuracy and adding features, but the v1 aims to deliver a functional, fast result with as little friction as possible.
Sources:[13][5][10][11][1][8]

AI Website Replicator – Development Roadmap (PLAN.md)
Overview
This development roadmap outlines the phased approach to building v1 of the AI-assisted website replicator. We break the work into phases corresponding to major components: architecture setup, web scraping, AI planning/coding, frontend UI, integration of results, and packaging. Each phase lists key tasks, along with rough estimates of effort in human developer hours and any significant machine/AI execution time. We also highlight which tasks can be accelerated by using AI agents (e.g., using Google’s Jules or Gemini for partial automation) and note major milestones.
The plan emphasizes rapid development and iterative delivery. Given the preference for speed-to-ship over exhaustive optimization, we schedule the minimal work to get a functional prototype, then plan improvements. Key milestones occur at the end of each phase, providing a checkpoint (often an internal demo or testable artifact) before moving on. This ensures we have a working vertical slice early and can adjust course as needed.
Phase 1: Project Setup & Architecture (Week 0.5)
Objective: Establish the project structure, choose tools, and get the development environment ready for both frontend and backend. Define how the TS frontend and Python backend will communicate.
    • Task 1.1: Tech Stack Confirmation and Repos Setup – Human: 4h. Decide on the exact versions of Next.js, Python, and any libraries (Playwright, etc.). Initialize two codebases: a Next.js app (TypeScript) and a Python package for backend. Set up a monorepo or separate repos as appropriate. Ensure both can be run locally. Milestone: A Git repository with basic Next.js app (hello world page) and a Python script that runs (print hello).
    • Task 1.2: Communication Interface Design – Human: 4h. Define the API contract between frontend and backend. Likely a REST endpoint like POST /api/replicate which accepts JSON (URL, options, branding data). Decide on data serialization (e.g., images will be Base64 encoded or uploaded separately). Document this interface. AI assist: Use Gemini or Copilot to draft an OpenAPI spec or interface outline (Machine: ~0.5h). Milestone: API spec documented (could be in README) and a stub handler in Next.js (if using Next API routes) or in backend to illustrate the request/response format.
    • Task 1.3: Infrastructure & Deployment Planning – Human: 3h. Determine deployment targets: e.g., Vercel for frontend, Google Cloud Run for Python. Write a minimal Dockerfile for the Python service. Configure environment variables for API keys (placeholder for now). Milestone: Able to deploy a “hello world” backend on Cloud Run (or similar) and call it from the local frontend (just testing connectivity).
Milestone Check: Basic Architecture in Place. We have the skeletal frontend & backend communicating. Next milestone will have actual scraping.
Phase 2: Scraping & DOM Processing (Week 1)
Objective: Implement the web scraping module to fetch a page (and linked pages up to depth 3), then process the DOM into a structured intermediate form. This phase yields the ability to input a URL and get a parsed representation of the site’s structure.
    • Task 2.1: Playwright Integration for Page Fetching – Human: 6h. Install and configure Playwright in the Python backend. Write a function to launch a headless browser, navigate to a given URL, and wait for the page to load. Ensure it captures dynamic content (maybe wait for network idle or a specific element). Implement error handling (time out after X seconds, etc.). Milestone: Given a URL, the backend can log the page’s title or HTML content (proof that scraping works).
    • Task 2.2: Multi-page Crawl – Human: 4h. Extend the scraper to discover links on the page and fetch a limited number of sub-pages. Use BFS up to depth=3. We’ll need to avoid external domains and large numbers of pages. Implement filters (same domain only, skip repeated links). This might involve queue logic and storing seen URLs. Milestone: For a small website, the scraper prints out the URLs it will scrape and fetches all within the depth limit.
    • Task 2.3: DOM to AST Conversion – Human: 10h, Machine: 2h. Develop a parser that takes the raw DOM (likely as an HTML string or Playwright’s DOM objects) and produces a structured AST. Outline the AST classes (Page, Section, Component, etc.). Implement heuristics: for example, identify header/nav by looking for <nav> or certain id/classes, segment the DOM by top-level sections (maybe use the visual bounding boxes via Playwright’s screenshot if needed). AI assist: Use an LLM to suggest component breakdown for sample HTML (Machine: use Gemini to analyze an example DOM and propose a JSON structure[14]). Incorporate those suggestions into code. Milestone: For a test HTML, the AST generator outputs a JSON hierarchy of components.
    • Task 2.4: Screenshot Segmentation (optional in v1) – Human: 0h (skipped or minimal). If time permits, integrate a method to take a screenshot and do basic segmentation (e.g., using OpenCV or a lightweight vision approach to find horizontal sections). This can complement DOM analysis for detecting distinct visual regions. Mark as optional; focus is on DOM first.
Milestone: Scraper & Parser Demo. Inputting a URL returns a structured representation of the site (e.g., printed to console). We verify on a couple of known sites (perhaps a simple Landing Page template and a documentation page) that the structure makes sense. At this point, we can proceed with AI planning using this structure.
Phase 3: AI Prompting & Planning Module (Week 2)
Objective: Leverage an AI (Planner Agent) to interpret the AST and produce a formal plan (specifications and tasks) for site generation. Set up prompt templates and iterate to ensure the plan covers all needed components. Also, define prompt templates for code generation (Coder Agent) in the next phase.
    • Task 3.1: Define Prompt Templates for Planning – Human: 6h. Write the initial version of the system and user prompts that will be sent to the Planner Agent. The prompt will include a description of the project and possibly a compact representation of the AST/site-map. For example: System: “You are a web development planning assistant. You will receive the structure of a website and produce a development plan… (include guidelines: use Next.js, etc.)”. User: “Site structure: [JSON AST]”. Then ask the AI to output a spec and plan. We’ll include in the system prompt instructions to output in Markdown with specific sections (so it mirrors Conductor’s style: e.g., “## Specs” and “## Plan with phases/tasks”). Milestone: A well-crafted prompt template stored in our code (with comments explaining each part).
    • Task 3.2: Implement Planner Agent Call – Human: 4h, Machine (AI execution): 1h per use. Integrate with the Gemini API to actually call the model with the prompt. Parse the output (which should be markdown text). Verify it can be converted into our internal task structures. This will likely require a few prompt tuning rounds: we test on a sample AST to see if the output is usable. AI usage: Each test call might take e.g. 30 seconds (depending on model). We budget ~1h total of model time for prompt tuning across samples. Milestone: Running the planner function yields a markdown plan we can programmatically interpret (e.g., list of components and tasks).
    • Task 3.3: Review and Refine Planning Output – Human: 4h. Evaluate the AI’s plan for edge cases: Does it include everything? Are tasks too granular or too high-level? Adjust prompt or do a minor post-process. E.g., if AI lists tasks like “Build header, build footer, build page layout…”, that’s good. If it misses something (like a special script), we may adjust by adding hints in the prompt. Also ensure the plan is in sync with our intended code structure (phases can mirror how we will implement with Coder). Milestone: Planner agent consistently produces an 85–90% correct plan for test sites.
    • Task 3.4: Functional Requirements Cross-check – Human: 2h. Make sure the plan includes tasks for branding substitution and assets. If not, we might append an instruction for that (like ensure one task is “Replace assets with user provided ones”). Also, we cross-check the plan against Conductor’s format so that if we output it as plan.md, it would make sense. Possibly incorporate Conductor’s track metadata if needed (though optional).
    • Task 3.5: Prepare Codegen Prompt Template – Human: 5h. In parallel to planning, draft the prompt for the Coder agent. This might be a function that, given a specific task (like “Build component X”), creates a prompt: System: “You are a coding assistant. You have context: [perhaps include relevant snippet of AST/spec]. You need to output code for Next.js component fulfilling this description… Use Tailwind, etc.” Possibly a one-shot example is included (few-shot) to guide format. We’ll also specify the output should be just code (or code with minimal explanation in comments). Milestone: A template for code generation prompt ready, though we won’t fully test it until next phase.
Milestone: AI Planning Complete. At this stage, given a parsed site structure, the system can produce a high-level development plan in markdown (or internal object). We likely generate a sample specs.md and plan.md for a test site as a deliverable (and we can include them in our repo for reference). Moving on, we have the blueprint to generate code.
Phase 4: Automated Code Generation & Assembly (Week 3)
Objective: Implement the code generation loop using the Coder agent. For each planned component/task, invoke the AI to produce code, then integrate that code into the project structure. Also handle supporting files (Tailwind config, etc.). By end of this phase, the system should be able to output a runnable project.
    • Task 4.1: Scaffolding the Next.js Project Structure – Human: 4h. Before generating code, programmatically create the base project files: e.g., initialize a package.json (we can template this), create a pages or app directory depending on Next.js version (likely using Next 13 with the App Router or the Pages router – decide and scaffold accordingly). Copy in static files like a default tailwind.config.js, postcss.config.js, globals.css with Tailwind base imports, and install shadcn/ui if needed (we may include its dependency in package.json). This can be done with simple file copy operations from a template folder. Milestone: The output directory has a skeleton Next.js project even before AI fills in components.
    • Task 4.2: Coder Agent Integration – Human: 8h, Machine: variable per generation. Implement the loop that goes through the plan tasks and for each invokes the AI to generate code. Likely we group tasks by type: e.g., first generate all shared components (header, footer), then pages, etc., to allow context reuse if needed. But a simpler approach is sequential. For each task, pass a prompt and get output. Then save the output to the appropriate file. We need to parse the AI response: ideally it’s just code (perhaps wrapped in markdown ``` for formatting). We’ll strip any extra prose. If the agent sometimes returns explanations, we may adjust the prompt to say “only code inside one markdown block”. AI compute: Each component generation might take ~10-20 seconds. If we have, say, 10 components/pages, one run is a few minutes of model time. We should design to possibly parallelize small tasks or at least not block unnecessarily (though sequential might be fine initially). Milestone: The system can produce code for a single simple component correctly via the AI and save it.
    • Task 4.3: Iterative Refinement & Error Handling – Human: 8h, Machine: 2h. Test the generated code by actually running npm run build or npm run dev. Likely there will be issues (syntax errors, missing imports). We need to handle those. Strategy: after generation, run a quick lint or compile check (maybe use tsc or Next’s build in a subprocess). If errors, capture them and possibly re-prompt the AI to fix (this could be an advanced step, maybe not fully in v1). Alternatively, log them for developer to fix manually. Given v1 time constraints, we might do a simple approach: ensure our prompt includes TypeScript type expectations to reduce errors and rely on manual debugging for any edge cases. We fix obvious issues by adjusting prompt or doing small string replacements. Milestone: Generated project builds successfully on a few test sites.
    • Task 4.4: Integration of Tailwind and shadcn UI – Human: 6h. Make sure generated code uses the imported components correctly. We might need to add import { Button, ... } from "@/components/ui/button" or similar in code if using shadcn. Provide the AI with those details in context. If needed, we can generate a base set of UI components by installing shadcn and copying their library into our project output (shadcn provides a script or we can pre-add a bunch of UI components like button, dialog etc.). Ensure Tailwind classes the AI uses are either from Tailwind itself or exist in shadcn components. AI assist: Possibly use AI to generate some Tailwind classes if needed. Milestone: In a test output, the styling is present and looks correct (colors, spacing align with original to a good degree).
    • Task 4.5: Asset Handling – Human: 3h. Implement the logic to insert user-uploaded assets. If we have the image files in memory or saved, place them in public/ folder of Next app. Replace references in code (e.g., any <img src="original-site-logo.png"> becomes <Image src="/logo1.png"> or similar Next Image component usage). Possibly generate an import if needed. This might involve storing a map from original image URLs to new filenames during the planning phase. Milestone: User images end up in the output and code references them.
    • Task 4.6: Final Assembly and ZIP – Human: 4h. Once all files are generated and saved to a temp directory, implement the zipping function. Use Python’s shutil.make_archive or similar. Stream the zip back in the API response. Ensure to exclude any secret info (no API keys in the output). Milestone: Hitting our API yields a downloadable zip containing the project.
Milestone: Basic Generation Complete. At this point, the pipeline from URL input to code output is functional. A major milestone demo: clone a small known site and deploy the result to see how close it is. We should have at least one example working (maybe a simple marketing page).
Phase 5: Frontend UI & User Experience (Week 4)
Objective: Build out the Next.js frontend around the backend pipeline. This includes the upload form, progress feedback, and results display. Also implement any user controls for customization (branding inputs, style toggles).
    • Task 5.1: Input Form Page – Human: 6h. Develop the main page where users enter the target URL and their choices. This form will have: URL field (with validation), file upload inputs (use a component that handles up to 5 file selection, showing thumbnails or names), and fields for text replacements (site name, maybe tagline – we can dynamically generate these fields after initial scrape or just include a couple of generic fields). Also a field or dropdown for color scheme if we offer that, and a free-text prompt input for additional instructions. Use React state to manage the inputs. Style the form using a simple Tailwind layout or a shadcn form component. Milestone: The front page is styled and interactive (but not yet calling backend).
    • Task 5.2: API Integration & Progress Indicator – Human: 8h. Connect the form to actually call our backend API. Likely we’ll create an API route on Next that proxies to the Python service (for CORS simplicity) – or call directly if CORS is allowed. Implement the request logic, including sending files (possibly convert to Base64 JSON or use FormData if our backend endpoint supports multipart). The response might be a stream (if we stream zip) or a job ID. For simplicity, we might have the frontend call and then long-poll status. Implement a progress UI: e.g., a modal or separate page that shows “Scraping... Planning... Generating header... etc.”. We can update this by either: periodically fetching status from backend (if we set that up) or by updating steps in frontend as we expect them to happen (optimistic approach). A more robust method: the backend could send progressive events via WebSockets or Server-Sent Events. Given time, we might do a basic polling: e.g., every 5 seconds check a status endpoint for that job. Implement the status route in backend (store job progress in a global dict for now). Milestone: Starting replication transitions to a progress screen and after completion it automatically proceeds to results.
    • Task 5.3: Results & Download Page – Human: 4h. Create a results screen that appears when the backend finishes. This page should show a summary: maybe list the pages that were generated (we can get that from the plan or output), any warnings (like “3 images were not replaced due to limit”), and a big download button for the ZIP. Also maybe an inline preview: we could show the homepage by reading the generated index.html – but since this is Next.js code, we’d have to either spin it up or render static. For v1, we might skip live preview and just stick to download. We ensure the download link triggers properly (since we likely got the zip as a blob response, we can create an object URL to download). Milestone: After generation, user sees a final screen with a working download link that indeed provides the zip file of their site.
    • Task 5.4: Branding UI and Validation – Human: 4h. Improve the form to handle branding inputs nicely. For example, if user uploads a logo, show it in an <img> preview. Validate that at least a URL is provided. If user tries to clone a huge site, maybe warn them. Also add front-end enforcement of file limit (no more than 5 files, and maybe file size limit). For color pickers or style options, use a simple component if possible. Milestone: The form is user-friendly and prevents obvious mistakes, guiding the user to provide necessary info.
    • Task 5.5: Styling & UX Polish – Human: 6h. Make sure the frontend looks professional (consistent spacing, responsive layout). Possibly incorporate a loading spinner or fun animation during progress. Write some copy text for the UI (titles, instructions) that clearly explain what the tool does and any disclaimers. Also ensure any errors from backend are caught: e.g., if backend returns error JSON, show it in an alert on the UI. Milestone: The UI/UX is clean and ready for an initial user trial.
Milestone: User Interface Complete. The entire flow from user input to result retrieval is now available through the web UI. We should be able to have a user (perhaps a team member) run through cloning a site without directly interacting with the code.
Phase 6: Testing, Optimization, and Launch (Week 4.5 to 5)
Objective: Test the end-to-end system on various sample sites, fix bugs, optimize performance where easy, and prepare for deployment and release.
    • Task 6.1: End-to-End Testing on Sample Sites – Human: 6h, Machine (AI): 2h. Pick 3–5 representative websites (small company homepage, a blog, a documentation page, etc.) and run the replicator. Check the outputs manually: do they render? Are structures correct up to depth 3? Note any failures or ugly outputs. For each, identify whether the issue is in scraping, planning, or coding. Then address them:
    • If scraping missed something, refine the parser.
    • If planning misinterpreted, adjust the prompt or add a post-correction step.
    • If code is broken, implement a fix or a retry in generation. AI assist: We can use an agent to review the output code for obvious errors (like run an LLM with “here is the code, find issues”). This could catch things we missed (Machine ~0.5h per site analysis). Milestone: Document results of tests and have fixes in place for all critical issues found.
    • Task 6.2: Performance Profiling and Speed-Up – Human: 4h. Analyze logs to see where time is spent. Perhaps the AI calls are the bottleneck (e.g., planning taking 30s and coding 2min). If planning is slow because we send the whole AST text, consider summarizing or sending fewer tokens. If code generation is slow per component, maybe try batching some prompts (though that’s risky). Also check if the headless browser is a bottleneck; possibly enable caching of assets or using a lower resolution screenshot if not needed. If any straightforward optimizations (like parallelizing page fetches or using the faster Gemini Flash model for code) are available, implement them and test that quality doesn’t drop much. Milestone: Achieved some reduction in total runtime (even 20% improvement is fine for v1). Confirm that under expected usage the process doesn’t hit timeouts or memory limits.
    • Task 6.3: Documentation & Comments – Human: 5h. Finalize all internal documentation. Ensure prompt templates in code are well-commented (each section of the prompt explained, as required). Write a README for the repo that explains how to deploy, how to run locally, and includes an example. Also include usage guidelines and the legal disclaimer. Milestone: Repo is documentation-complete.
    • Task 6.4: Deployment Setup – Human: 3h. Prepare production deployment. Set environment variables (Gemini API key, etc.) in Vercel and cloud service. Deploy the latest frontend and backend. Run a final smoke test in production environment (perhaps with a small site) to ensure all services and integrations (especially API keys and file handling) work outside local environment. Milestone: v1 is deployed and accessible (maybe initially to a closed group or at a secret URL for safety).
    • Task 6.5: Buffer and Fixes – Human: 4h. This time is reserved for any last-minute fixes or adjustments discovered during final testing or deployment. Could include scaling config changes (e.g., increasing memory on the container if needed) or quick UX changes (like adjusting text or adding an extra warning if something was confusing to testers).
Milestone: v1 Launch Ready. All critical functionality works, initial performance is acceptable, and the product is documented and deployed. We consider this the completion of the development cycle for version 1.0.
Estimated Timeline and Effort
    • Phase 1: ~1–2 days (8–12 human hours)
    • Phase 2: ~4–5 days (20–24 human hours, plus some AI assistance for parsing logic)
    • Phase 3: ~2–3 days (15–20 human hours, plus ~1–2 hours AI usage for prompt iteration)
    • Phase 4: ~4–5 days (25–30 human hours, AI usage depends on number of components generated during dev/testing – perhaps ~3–5 hours total model time across iterations)
    • Phase 5: ~3–4 days (20–24 human hours)
    • Phase 6: ~2–3 days (15 human hours for testing/tweaks)
Overall, we anticipate roughly 80–100 hours of focused development work for a single developer, spread over about 4–5 weeks elapsed (accounting for testing and adjustments). Machine hours (AI processing time and automated tasks) are relatively small compared to human design/debug time – maybe on the order of a few hours total of model computation for development/testing runs. Individual replication jobs for users will typically consume a few minutes of machine time each.
These estimates assume no major unforeseen issues. Given the complexity (especially the AI’s unpredictable output), we allocate extra buffer in Phase 6. Also, leveraging AI during development (with Conductor and Jules) can offset some human coding time – possibly reducing the coding effort by ~15–20% on tasks where the AI can produce boilerplate or parse code. For instance, using Jules to implement the AST data classes or to generate Tailwind config could save a few hours[22]. We will use those tools where prudent to keep momentum.
Key Milestones
    • Milestone A: Basic end-to-end flow with dummy data (Phase 1 done) – Week 1. We can simulate a job through the system (without real AI) and see the pieces connect.
    • Milestone B: Successful scraping & AST output for real site – Week 2. Demonstrates backend’s capability to parse a site structure (foundation for AI input).
    • Milestone C: AI planning + sample plan.md generated – Week 2. We have the AI producing a plan we’re happy with (even if code gen isn’t done yet).
    • Milestone D: First full site clone output (local) – Week 3. The system generates a simple site’s code and it runs. This is the vertical slice proving the core idea.
    • Milestone E: Usable web UI for replication – Week 4. A user can go through the UI, and the backend performs the clone, returning a downloadable site. There may still be rough edges, but it works externally.
    • Milestone F: Polished Release Candidate – Week 5. After testing and tweaks, the app is ready for initial users or a beta launch. It meets the MVP criteria (85–90% structural accuracy on tested sites, branding integration, performance within limits).
At each milestone, we’ll solicit feedback (from team or friendly users) and adjust priorities if needed. By focusing on an MVP feature set in each phase and deferring nice-to-haves, we ensure that the core value – quickly cloning a site’s structure and rebranding it – is delivered on time.
Sources:[19][9]

AI Website Replicator – Development Checklist (TODO.md)
This checklist breaks down the remaining tasks into specific, actionable items organized by component/module. Developers can use this as a to-do list, marking items as completed. Each section corresponds to a major subsystem: Frontend UI, Python Backend (scraping & logic), AI Integration, Project Generation, and Others (testing, docs, etc.).
Frontend (Next.js)
    • [ ] Create Next.js App: Initialize a Next.js TypeScript project (if not done). Set up basic project structure (pages or app directory).
    • [ ] Design Input Form: Implement a page with a form to input the target URL and user options:
    • [ ] URL input field with proper validation (must be a valid http/https URL).
    • [ ] File upload fields (use an input or file drop zone) allowing up to 5 image files. Show preview or list of file names.
    • [ ] Text inputs for customizable content (Site name, maybe tagline, etc.).
    • [ ] (Optional) Color picker or dropdown for theme selection (light/dark or primary color choice).
    • [ ] Additional instructions textarea (for user prompt to AI).
    • [ ] Submit button that triggers the replication process.
    • [ ] Form Validation: Add client-side checks: required fields, correct file types, limit number of files. Disable submit until inputs are valid.
    • [ ] API Integration: Connect form submission to call the backend API endpoint:
    • [ ] Assemble form data (URL + settings + files). If files are small, Base64 encode into JSON; otherwise, use fetch with FormData multipart.
    • [ ] Display an error to user if API call fails immediately (network error, etc.).
    • [ ] Progress Feedback UI: Create a component or page to show progress:
    • [ ] Indicate steps like "Scraping site...", "Planning site structure...", "Generating code for X..." in sequence.
    • [ ] (Basic implementation) Immediately show a static message like "Cloning in progress, this may take a couple of minutes..."
    • [ ] (Enhanced) Poll the backend for status updates (e.g., check /status?jobId= periodically). Update a progress bar or text accordingly.
    • [ ] Ensure the UI remains responsive and user can cancel if needed (maybe provide a cancel button that signals backend if implemented).
    • [ ] Result Display Page: After completion, show results:
    • [ ] Success message and summary (e.g., "Site cloned successfully! Pages generated: Home, About, Contact.").
    • [ ] Download button or link for the ZIP file. Implement logic to download the file returned by backend (maybe via a blob URL or by redirecting to a file URL).
    • [ ] If feasible, show a small preview (even just an image screenshot of the homepage if we have it, or an embedded iframe pointing to a deployed version if that exists).
    • [ ] Error handling: If the backend finished with an error status, show an error message with possible causes (e.g., "Cloning failed: Site too large" or "Unknown error – try a different site.").
    • [ ] Branding and Styling of UI: Polish the frontend's look:
    • [ ] Use Tailwind CSS (already set up via Next.js if using shadcn components or include Tailwind config).
    • [ ] Ensure the form and progress page are mobile-friendly (responsive).
    • [ ] Add a header or title to the app (branding of our tool) and maybe a brief description for first-time users.
    • [ ] If using shadcn/UI or Radix components for better design (e.g. for dialog, progress), integrate them.
    • [ ] Connect Frontend to Backend URL: Configure environment for the backend API URL (if calling external service). In development, it might be http://localhost:8000/replicate; in production, some Cloud Run URL. Use Next.js environment variables for this.
    • [ ] Test Frontend in Isolation: Test the form with dummy data (e.g., intercept the submit to not call backend but simulate a response) to ensure UI flows correctly from start to progress to result.
Backend – Scraper & Parser (Python)
    • [ ] Environment Setup: Ensure Playwright (or chosen headless browser) is installed and that the runtime environment has required dependencies (Chromium, etc.).
    • [ ] Scrape Single Page: Implement function fetch_page(url) -> (html, screenshot):
    • [ ] Launch headless browser (Playwright) and navigate to url.
    • [ ] Wait for page load (network idle or a few-second delay).
    • [ ] Extract page content: get page.content() (HTML) and possibly take a full-page screenshot (if needed for segmentation).
    • [ ] Handle exceptions (navigation timeout, page crash, etc.) – return an error status if fails.
    • [ ] Unit-test this on a sample URL (maybe allow running a CLI to print the title or length of HTML).
    • [ ] Crawl Links (Depth-Limited): Expand fetch_page into fetch_site(url, depth):
    • [ ] Parse the initial page HTML for links (<a href> tags). Use something like BeautifulSoup or Playwright’s DOM methods.
    • [ ] Filter links: only same-origin, no mailto, no huge query params if possible.
    • [ ] For each link (breadth-first): fetch the page (using the above function), add to results. Limit by depth and a max pages count (maybe max 10 pages to avoid overload).
    • [ ] Store results in a structure like {url: html} for each fetched page.
    • [ ] Ensure not to fetch the same URL twice (keep a visited set).
    • [ ] Test by running on a small site with a couple of subpages.
    • [ ] DOM to AST Conversion: Implement parse_dom_to_ast(html) -> AST:
    • [ ] Use an HTML parser (BeautifulSoup, lxml, or even Playwright’s evaluate script) to build a DOM tree object.
    • [ ] Define classes or dict structure for AST nodes (Page, Section, Component).
    • [ ] Identify major sections:
        ◦ [ ] If screenshot available: possibly use an approach to segment by visual breakpoints (optional, skip if not enough time).
        ◦ [ ] Use DOM heuristics: e.g., find <header> or <nav> as likely top menu; <footer> for bottom; main content by <main> or large <div>s.
        ◦ [ ] Within main content, break into sub-sections by headings or containers. For example, multiple <section> tags, or large <div> siblings, each could be a section.
    • [ ] For each section, list contained elements: text blocks, images, lists, buttons, forms, etc., with their attributes.
    • [ ] Extract important style info:
        ◦ [ ] For container sections: background color or image (from CSS or inline styles).
        ◦ [ ] For text: any strong styling (like a class that indicates a font or color).
        ◦ [ ] Maybe capture classes to feed to AI as clues (like class names often hint at purpose, e.g., “hero-banner”).
    • [ ] Represent the AST in JSON for debugging.
    • [ ] Test parsing on saved HTML of known layout, see if it produces a reasonable component hierarchy.
    • [ ] Infer Reusable Components: (Optional) If the site has repeating patterns (like multiple cards with same structure), mark them as a single component type in AST. E.g., identify duplicates by structure. If time permits.
    • [ ] Logging & Limits: Add logging statements to the above:
    • [ ] Log when a page is fetched (URL, status, maybe size).
    • [ ] Warn if skipping links due to depth or count.
    • [ ] Log the summary of AST (how many pages, sections).
    • [ ] If any page’s HTML is extremely large (over some KB), consider trimming or warning.
    • [ ] Error/Edge Handling:
    • [ ] If the target site requires login or is not reachable, return an error early (with message “Page not accessible”).
    • [ ] If the site uses infinite scroll or has a lot of dynamic content, our approach might miss it – note this as a known limitation (we won’t solve in v1).
    • [ ] Respect robots.txt if needed (for now, user-driven so perhaps skip).
Backend – AI Integration & Agents
    • [ ] Gemini API Client Setup: Install Google’s generative AI SDK (google-generativeai) or prepare HTTP calls to Vertex API. Obtain API key and set up auth (possibly via environment var for API key or service account).
    • [ ] Prompt Template Definition: In code or separate template files, write out the base prompts:
    • [ ] Planner Prompt: Includes instructions for analyzing site structure and outputting a development plan. Use placeholders for inserting project context (like AST summary, tech preferences).
    • [ ] Coder Prompt: Template for generating code for a given component. Include placeholders for component description, any style guidelines, etc. Possibly include an example format.
    • [ ] Comment each section of prompt (why we include certain instructions).
    • [ ] Planner Agent Implementation:
    • [ ] Create a function generate_plan(ast) -> planMarkdown that:
        ◦ [ ] Prepares the prompt by summarizing AST. Perhaps limit length: include page names, section names, omit raw text beyond maybe headings to save tokens.
        ◦ [ ] Calls the model (with model=Gemini e.g. “gemini-3”) and retrieves the response.
        ◦ [ ] Implements retry logic: if API times out or returns incomplete output, try once more or on smaller model.
        ◦ [ ] Parse the markdown output to separate spec and plan if needed. Possibly store as structured data (like a list of tasks with descriptions).
        ◦ [ ] Log token usage or model info (the API might provide usage info).
    • [ ] Test this function with a small fake AST (or real AST from previous phase on a simple page). Print the result to check format.
    • [ ] Coder Agent Implementation:
    • [ ] Function generate_component_code(task) -> code:
        ◦ [ ] Prepare prompt from template, filling in the component description from plan or AST (e.g., “Header with logo and nav links: [list of link names]”).
        ◦ [ ] Call model (maybe smaller or same model). Possibly use gemini-3-code if such exists, or instruct properly for code.
        ◦ [ ] Receive response and extract code. If the response is in markdown with triple backticks, strip them.
        ◦ [ ] Save or return the code string.
        ◦ [ ] Basic error handling: if code seems incomplete or too short, maybe log a warning or attempt one more call (perhaps with a simplified prompt).
    • [ ] Prepare context that stays constant across calls: e.g., system prompt that includes global instructions (“use Tailwind, etc.”) can be reused. Use that to avoid repeating tokens. If API allows a conversation, we could keep a single session for multiple components (but be cautious about context length). Possibly simpler: treat each component independently with its own prompt to keep things simple in v1.
    • [ ] Test on a dummy task (like “generate a simple footer with 2 links”). Check if output is syntactically correct.
    • [ ] Multi-Agent Orchestration Logic:
    • [ ] Implement the overall orchestration in the main replicate_site function:
        a. Call scraper to get AST of all pages.
        b. Call planner agent to get plan (spec + tasks).
        c. Initialize an empty project directory structure (in a temp dir).
        d. Process each task from plan:
        e. [ ] Determine file or component name for task. E.g., task “Build Header component” -> file components/Header.tsx. Task “Build Home page” -> pages/index.tsx.
        f. [ ] Call coder agent with task description to get code.
        g. [ ] Save code to appropriate file. If file belongs to components or pages, ensure directories exist.
        h. [ ] If multiple tasks refer to same file (maybe not in our plan structure, but just in case, avoid overwriting by merging or skipping duplicates).
        i. After all tasks, assemble other necessary files:
        j. [ ] Write package.json (with dependencies: next, react, tailwind, shadcn/ui etc.).
        k. [ ] Write Tailwind config (populating theme colors if available from AST/branding).
        l. [ ] Write a basic README.md describing how to run the project.
        m. [ ] If Conductor integration: write conductor/product.md, plan.md etc. from the plan and spec.
        n. [ ] Copy user-uploaded images into public/ and ensure code references (adjust code if necessary to point to correct paths).
        o. Zip the project folder.
    • [ ] Manage state: Use a job ID to track progress. Possibly update a global dict or use an async task queue. Mark steps as done so that the /status endpoint can retrieve it.
    • [ ] If any exception in generation (e.g., one component fails), decide on fallback: maybe skip that component and note it in README (“TODO: component X could not be generated”). We prefer not to crash the whole process if one part fails.
    • [ ] Timeouts: ensure that if the AI takes too long, we have some timeouts in place to avoid hanging (this might rely on API’s built-in timeouts or we set our own using asyncio or threads).
    • [ ] Testing with AI (small scale): Do a dry run on a very simple site (could even be a static HTML we have locally) to see end-to-end if the above logic yields output. Because AI calls may be slow/costly, start with one or two tasks or use a small model or stub. Possibly stub the AI calls with hardcoded output for initial integration test. Once the flow is confirmed, do a real call.
    • [ ] Tool/Resource Limits: Implement any needed limits for AI calls:
    • [ ] For context size: if AST text is huge, truncate or summarize before sending to planner (maybe just list top 5 sections and say “...and X more sections”).
    • [ ] Rate limiting: ensure we don’t make too many parallel API calls (in v1, probably sequential anyway).
    • [ ] Possibly use the Adaptive model strategy in future; not in v1 unless trivial.
Project Generation & Output
    • [ ] Template Files: Prepare template versions of certain files to include:
    • [ ] package.json with placeholders for project name (to be replaced with user’s site name) and dependencies.
    • [ ] tailwind.config.js with a basic config. We might programmatically insert color values into the theme section if we have them.
    • [ ] postcss.config.js and globals.css with Tailwind directives (these could be static files to copy).
    • [ ] .gitignore maybe (so that the user’s repo doesn’t commit node_modules, etc.).
    • [ ] If using shadcn: maybe a pre-built set of UI components (or let the AI generate as needed). Possibly run npx shadcn-ui init manually and copy the resulting components as a starting point.
    • [ ] Asset Handling: Ensure that for each user-uploaded file:
    • [ ] Save the file to an assets/ or public/ directory in the output folder. Use a stable name (like logo.png, or if multiple, asset1.png, asset2.png etc.).
    • [ ] Keep track of mapping from original image in site to new file. The plan or AST might mark where the original logo was (e.g., <img src="original_logo.png">). Replace those references in code. Possibly easiest is to search in generated code for the original image URL (or parts of it) and replace with our new file path.
    • [ ] If user provided fewer images than the site had distinct images, just leave some original images URLs in place (or download those original images into the zip for completeness?). To be safe, we could download critical images from the site (like background images in CSS) and package them as well, to ensure the cloned site is fully self-contained. That might be advanced, but consider at least for main images.
    • [ ] Validate that if a user image is e.g. SVG and the code expects img tag, it will be fine (it should).
    • [ ] Brand Text Replacement: Implement a simple content substitution:
    • [ ] If user provided “Site Name”, then after generation, do a pass on all output files (pages and components) to replace occurrences of the original site’s name with the new name. The original name might be detected in AST (e.g., title text). Use a case-sensitive replace carefully to not accidentally replace in code variables unless intended.
    • [ ] Similar for tagline or other provided texts.
    • [ ] If the user provided a primary color choice, and if we had integrated that, ensure Tailwind config uses it (or apply a CSS variable in globals.css). Possibly also adjust any inline styles or class names in code if needed (since ideally code uses the Tailwind text-primary which we map to that color).
    • [ ] ZIP Packaging: Use Python’s zipfile or shutil:
    • [ ] Collect all files from the temp project directory.
    • [ ] Create a zip in memory or disk.
    • [ ] Ensure file permissions are normal, and no unwanted files included (skip things like Playwright browser cache if any, etc.).
    • [ ] Provide the zip bytes to the API response. If API is synchronous, maybe base64 encode or send as binary with appropriate headers (Flask/FastAPI can stream file response). If asynchronous (job queue), we might store the zip in a temp location for download. For now, assume direct return (taking care with memory).
    • [ ] Clean Up: After sending zip, delete the temp project directory to free space (especially if on a server with limited storage). Manage any in-memory objects (clear them or rely on process exit).
    • [ ] Post-Generation Conductor Files: Optional, if we decide:
    • [ ] Write the specs.md and plan.md we got from the Planner agent to the output. These provide the user with the AI’s perspective of the project.
    • [ ] Possibly also include our todo.md in the project, which might list any manual steps the user should do (like "Replace API keys" or "Complete form handling code if any"). Could auto-generate some points if we detect placeholders.
    • [ ] Provide a setup.md that could be used with /conductor:setup (which might include product description, tech stack, etc. – we have that info to some degree).
Workflow & Miscellaneous
    • [ ] Testing Infrastructure: Develop a few test cases:
    • [ ] Unit tests for parsing functions (input HTML -> expected AST structure).
    • [ ] If feasible, automated integration test: spin up the whole pipeline on a known small site and assert that output contains expected files (e.g., known text appears in generated page). This could be tricky with nondeterministic AI output, but we can at least test the non-AI parts deterministically.
    • [ ] Performance Checks:
    • [ ] Time the major steps (scrape, plan, codegen). Log durations. Make sure no step exceeds reasonable time for typical input.
    • [ ] If something is slow (e.g., Playwright default navigation is slow), consider optimizations (like disable loading images/scripts in headless if not needed for layout – might improve speed). This can be done via Playwright page route interception if needed. Possibly skip for v1 if not trivial.
    • [ ] Error and Edge Handling Checklist:
    • [ ] Try a URL with no CSS (very plain) – does it still create a decent output?
    • [ ] Try a URL that requires JS (like a React app) – Playwright should handle it, but ensure we wait enough for content.
    • [ ] Try a non-English site (just to see if any encoding issues or model issues – probably fine).
    • [ ] If the user uploads no images, ensure the system still works (just keeps original images or replaces none).
    • [ ] If user inputs an invalid URL, ensure backend returns error and frontend shows it.
    • [ ] Large site scenario: if site has 20+ pages, likely to exceed our scope – ensure we cut off and warn user (maybe implement a hard page count limit like 10 and mention it).
    • [ ] Documentation and Comments:
    • [ ] Ensure that all prompt strings in code have comments explaining their purpose and any placeholders.
    • [ ] Comment complex logic in parser or generation steps for maintainability.
    • [ ] Write or update README.md for the code repository explaining how to run the backend and frontend, how to set environment variables (API keys, etc.).
    • [ ] Double-check that the user-facing text (in UI or output README) is clear and free of typos.
    • [ ] Deployment Prep:
    • [ ] Write a Dockerfile for backend if not done (including installing Playwright browsers, etc.).
    • [ ] Set up Vercel project for frontend deployment and verify build (include any build command to generate types or such).
    • [ ] Test backend and frontend integration in a staging environment or locally with production-like settings (simulate by running backend on a cloud VM and pointing frontend to it).
    • [ ] Plan for API key secure storage (e.g., in Vercel/Cloud Run secrets).
    • [ ] Legal/Disclaimer:
    • [ ] Add a disclaimer in the frontend (maybe footer or on results page) reminding about copyright/ToS compliance.
    • [ ] Possibly include a copy of license info for any templates or libraries (shadcn/ui license in output, etc.).
Each item above can be checked off as development progresses. The list ensures no major aspect is forgotten in delivering the MVP. Developers should commit and push regularly, and update this TODO to reflect completion. Non-critical optional tasks are marked, which can be deferred if time is short. After all essential boxes are checked, the project should be ready for the v1 release.

AI Website Replicator – Workflows & Operational Details (WORKFLOWS.md)
This document describes both developer workflows (how to develop, test, and release the system) and the internal agentic workflows that power the AI coordination. We cover data flows between the TypeScript front-end and Python back-end, how AI prompts are structured and where humans can intervene or override, how user-provided branding flows through the system, and the development lifecycle processes (dev, test, release).
Data Flow: TypeScript–Python Serialization
Our architecture splits the application logic between the Next.js frontend (TS) and the Python backend. Data exchange must be serialized (likely via JSON over HTTP). Here’s the format and steps:
    1. User Input Capture (Frontend): The Next.js UI collects:
    2. url (string): The target website URL to replicate.
    3. assets (array of files): Up to 5 image files (logo and other images) uploaded by the user.
    4. siteName (string, optional): New site name/title (if provided by user).
    5. tagline or other texts (optional strings): Additional text customizations.
    6. styleInstructions (string, optional): A free-form prompt for stylistic guidance to the AI.
These inputs are held in React state until submission.
    1. Request Serialization: On submission, the frontend constructs a JSON payload. The image files are converted to a transmittable form. We have two options:
    2. Inline Base64 JSON: Convert each file to a base64-encoded string and include it in JSON (e.g., images: [{filename: "logo.png", content: "<base64>"}]). This keeps it single-request but can bloat the JSON.
    3. Multipart Form: Use an HTML form submission or FormData via fetch to send files as binary and other fields as text. The Python backend would parse a multipart form.
For simplicity, v1 chooses the JSON approach for small files (with a size check). The payload JSON might look like:
{
  "url": "https://example.com",
  "siteName": "My New Site",
  "tagline": "Just another awesome site",
  "styleInstructions": "Use a playful tone and bright colors.",
  "assets": [
     {"name": "logo.png", "data": "data:image/png;base64,iVBORw0KG..."},
     {"name": "hero.jpg", "data": "data:image/jpeg;base64,/9j/4AAQSkZJR..."}
  ]
}
The data URI scheme here includes the MIME type for clarity.
    1. API Call (Frontend): The JSON is sent via fetch to the backend endpoint (e.g., /api/replicate which is either a Next.js API route or directly the Python service URL). We prefer to have Next.js forward the request to the Python service (acting as a proxy) to avoid dealing with CORS in v1. This means the Next API route will receive the request, then use node-fetch or Axios to POST to the actual Python endpoint (configured via env var).
    2. Reception and Parsing (Backend): The Python server receives the request. If it's an API framework like FastAPI:
    3. It will parse the JSON automatically into a Python dict. If assets are base64 strings, it will handle those as strings. (If we used multipart, frameworks can handle file saves; in JSON method, we manually decode base64).
    4. The backend then decodes each assets.data: stripping the data URI prefix and base64-decoding into binary. It saves these bytes to a temporary directory or keeps in memory as needed. Filenames provided are sanitized (we'll remove any path components for safety, just use the base name).
    5. It constructs a job context: including the target URL, user text inputs, and references to the saved images.
    6. Status Updates (Backend → Frontend): Because replication can take time (possibly 1-3 minutes), we implement a basic status mechanism:
    7. When the job starts, the backend generates a job_id (e.g., a UUID).
    8. It immediately responds to the initial request with a JSON containing { "jobId": "<id>", "status": "started" } (or possibly we keep the HTTP connection open and stream, but that’s more complex).
    9. The frontend, upon receiving this, navigates to a progress page that begins polling. The Next.js app has an endpoint (or uses the same Python service’s status endpoint) like /api/status?job=<id>.
    10. The Python backend maintains a global (or in a database) job status dictionary: e.g., status[job_id] = {"phase": "scraping", "progress": 20}. As it goes through phases, it updates this.
    11. The polling endpoint simply returns the status for that job as JSON, e.g., { "phase": "generating code for footer", "progress": 60 }.
    12. The front-end uses this to update a progress bar or text. We categorize phases: e.g., Scraping (0-20%), Planning (20-30%), Generating components (30-90% distributed per component), Packaging (90-100%).
    13. When done, status might be { "phase": "completed", "progress": 100, "downloadUrl": "/api/download?job=<id>" }.
    14. The front-end then stops polling and triggers the download (navigating the user’s browser to that download URL or using fetch to get it as blob and then create a link).
This decouples the long process from the initial request, avoiding timeouts. (Alternatively, we could have initially responded only when done, but on Vercel/Serverless that’s risky due to time limits).
    1. Result Download: The frontend either automatically initiates download or provides a button. The downloadUrl might hit a Next.js API route that proxies from Python storage, or the Python service itself could serve the file. In v1 for simplicity, we may have the Python service store the zip in memory (or disk) and the Next front-end fetch it via an API call:
    2. Possibly we skip proxy and just provide a direct link to a signed URL or the cloud run URL (with job token). That requires CORS; might handle if properly configured (the Python can allow GET from our domain).
    3. Implementation decision: We might choose to stream the zip back in the initial request (which complicates showing progress). Instead, using job status + separate download is cleaner.
    4. Data Format Between Agents (Internal): Within the backend, the Planner agent produces a plan in markdown (text). We actually want a structured representation to feed the Coder. How we do this:
    5. Possibly instruct the planner to output JSON (but LLMs sometimes make errors in JSON). Given Conductor’s approach, we might accept a markdown and parse it.
    6. We define a simple format: for example, the plan section could be bullet points or a checklist of tasks. We can parse lines that start with - or 1. as tasks.
    7. Alternatively, we might have the planner output JSON explicitly (like an array of {file: "Header.tsx", description: "header with logo..."}). If we do that, we ensure to validate JSON with a regex or parse library.
    8. For MVP, markdown parsing is okay: our code can extract the tasks from the plan text reliably (we can include markers in prompt like “Tasks:\n1. …\n2. …”).
    9. The format to send to coder then is a textual description of each task (the coder doesn’t necessarily need JSON; a nicely formatted bullet could suffice as part of its prompt).
    10. Front-End to Back-End Data Examples:
    11. Status Poll Response: e.g., {"jobId":"1234", "phase":"Generating homepage", "progress":45}.
    12. Error Handling: If backend fails mid-job, it can set status {"phase":"error", "errorMessage":"Site too large to process"}. The frontend will detect and show that message. The download step won’t be offered.
AI Prompting & Multi-Agent Collaboration
Our system uses two primary agent roles: PlannerAgent and CoderAgent. They collaborate in sequence:
PlannerAgent Workflow: - Input: A summary of the scraped site structure and user’s goals/preferences. This includes: - Project context: “User wants to clone [Site Name or URL]. The site has X pages (list them). The user’s new site name is Y. They want these style changes: [if any].” - Technical stack info: “Use Next.js 13 with Tailwind CSS and shadcn/ui components. Use TypeScript. Output plan in markdown.” - Possibly some instructions to ensure format: e.g., “Begin by listing product goals, then a breakdown of features, then a task list.” - Output: The agent produces text which we expect to have sections: 1. Product spec/vision (e.g., high-level goals, maybe restating user’s intent). 2. Feature list / components (e.g., key pages and components to build). 3. Plan / tasks – a to-do list to implement those features.
We instruct it that the tasks should be actionable chunks (like “Implement Header component”, “Create About page with team section”, etc.)[23][24].
    • Internal format: Once we get this markdown, our backend might break it into:
    • spec_doc (the first part) – could be stored or included in output as specs.md.
    • tasks (the list) – which we’ll iterate for code generation.
    • Multi-agent aspect: PlannerAgent is a single-shot in our system, not iterative with another agent. However, we can consider the human developer as an overseer agent at this stage:
    • Manual override opportunity: We can allow a developer (or technically the end-user if we expose it) to review the plan before proceeding. In v1’s automated flow, we skip this to save time. But during development or debugging, a developer can log the plan and adjust it or feed a corrected plan to the next step. This is analogous to Conductor’s “review plans before code is written” philosophy[1][25]. If something looks off in the plan, we can intervene by editing the plan text or adjusting the prompt and rerunning.
    • In a future UI, one might show the user “Proposed Plan: [list tasks]. Proceed or edit?” – that’s an optional manual injection point. For now, we assume auto-approval.
CoderAgent Workflow: - Input per task: We feed it one task at a time. The prompt includes: - System instructions: Reminding it of the coding standards (TypeScript, Next.js, using given libraries, keep consistent style, output only code). - Context from spec if needed: Possibly a brief context like “Overall, the site is a blog. You are now coding the Header component which should be consistent with the overall style (light theme, uses primary color, etc.).” We can include the site name for context (maybe if the code includes text, like site name in header). - The specific task description: e.g. “Task: Implement the Header component with a logo on the left and navigation links (‘Home’, ‘About’, ‘Blog’) on the right. Use the Navbar component from shadcn/ui if applicable, or build with <header> tag and Tailwind CSS classes. Ensure it is mobile-responsive.” - Tools info: We might mention “You have access to the following UI components: [list of shadcn components]” to guide usage, if needed. - We could also slip in relevant AST snippet: for example, if the AST had specifics (like actual HTML snippet of the header), provide it: “Original HTML snippet for reference: <header><img src="logo.png"/><ul><li>Home…” – this can help accuracy.
    • Output: Ideally just the code for that component/page. We encourage the agent to produce a complete file content (including imports at top, component definition, any export default, etc.). We tell it not to include explanations. If it does produce some explanation, our code will strip it or ignore everything outside code fences.
    • Post-process & multi-turn: If the code is incomplete or has an error, we have a potential multi-turn scenario:
    • In v1, we largely expect the AI to get it mostly right. If we detect a clear error (like it calls a component that doesn't exist or misses an import), we could feed back the error as a new user message: “The code didn’t compile because XYZ. Please fix it.” This would utilize the same Coder agent context. This essentially creates a mini multi-agent loop: the coder agent acting, then an “evaluator” (our compile check) giving feedback, then coder agent revising.
    • Given time constraints, we might not implement a full loop in v1, but design is such that it’s feasible. For now, any critical fixes we might implement via string replacement or leave as TODO comments in output.
    • If a component relies on another (e.g., page uses the Header component), ensure we generate Header first, so by the time we generate the page, we can instruct “use <Header /> from ‘../components/Header’”.
    • Multi-agent concurrency: In theory, we could generate independent components in parallel by spawning multiple model requests at once (e.g., header and footer simultaneously). But since the model usage is sequential in our pipeline (and to keep context easier), we do one by one in v1. Concurrency can be considered if needing to speed up, but watch out for rate limits.
    • Manual prompts injection: Developers can adjust the code generation prompt templates if output quality is not as expected. For example, if the agent tends to omit Tailwind classes for spacing, we can add instruction “Ensure to preserve spacing similar to original, using Tailwind utility classes for margin/padding.”
Multi-Agent Collaboration: In our design, “collaboration” is mostly sequential hand-off (planner then coder). We do not have them actively converse with each other beyond what’s encoded in the plan output. In advanced setups, we might have used something like the Planner agent generating a plan, then asking the Coder agent if it has questions. We’re not doing that explicitly – instead, we rely on giving enough context to coder.
However, the system (especially with Conductor integration) is conceptually similar to orchestrating multiple agents with shared context: - The shared context is the plan + spec docs, which act like the “memory” both agents refer to (the coder essentially refers to tasks that come from the planner). - We ensure that context consistency by programmatically feeding relevant pieces; since we control both ends, the collaboration is tightly coordinated rather than free-form chat.
Tool Use by Agents: - The agents have “tools” implicitly in the environment: The coding agent in Gemini might have abilities like writing to files, searching web, etc., if invoked via a CLI. But in our setup, we aren’t delegating those (the Python backend itself handles file writes and we did web fetching already). - So the AI’s job is simplified to pure generation using provided info. No browsing or external calls by the agent (that’s why we do scraping ourselves). - This reduces errors from the agent and keeps it deterministic given input.
Context Length and Partitioning: - The scraped content (DOM, text) can be very large. We do not feed raw HTML of entire pages to the agent due to token limits (even though Gemini 3 can handle huge contexts, it’s expensive). Instead: - Planner sees a summarized structure: e.g., instead of raw HTML, we provide “Page Home: contains Header, Hero section (heading ‘Welcome to X’), Features section (grid of 3 items), Footer.” Essentially compress the site info into a outline form (which the AST helps with). - Coder for a component might see a snippet of original HTML for that component only (to copy any menu items, etc.). - We manage context to avoid overflow: if a page has a ton of text (like a blog content), maybe we don’t fully include it in prompt. We might just mention “...and main content of length ~2000 words, which you can use Lorem ipsum for now.” or instruct the agent to output a placeholder for large text. - The developer can override this if needed by editing the AST or summarizing differently.
Quality and Reasoning Depth: - We instruct the agent to not hallucinate structure that wasn’t in AST – basically follow the plan. This is enforced by giving clear tasks. - If the agent starts deviating (e.g., adding an extra UI element not present), that’s a risk. But since we describe tasks concretely, it should stick to it. - We have set the model to a high reasoning depth for planning (to deeply analyze structure)[26], but for code, perhaps a medium setting to get faster response. This is done either implicitly by model choice (Flash vs Pro) or explicitly if API supports a “thinking level” parameter. - Because the tasks are mostly straightforward coding, a shallow reasoning (focus on accuracy, not overthinking) might be fine.
Manual Overrides and Prompt Injection Points
Even though v1 aims to be fully automated, there are strategic points where manual input or overrides are possible:
    • Pre-Planning Override: A developer could intercept before running the Planner. For example, after scraping, inspect the AST. If the AST missed something or included irrelevant elements, a developer can manually tweak it (or write a quick patch in code). This is especially relevant in development phase for fine-tuning heuristics. In the running product for end-users, this is not exposed (unless we built an advanced mode UI in future).
    • Plan Approval/Edit: As mentioned, one could present the plan to the user or a developer to edit. In Conductor’s philosophy, reviewing the plan keeps the human in charge[27][25]. For now, we auto-approve. However, we do log it, so if something goes wrong downstream, developers can check “was the plan sensible?”.
    • Prompt Injection by User: The user’s “styleInstructions” is essentially a safe prompt injection point. We design the prompt to incorporate it as user instruction to the Planner (and possibly to Coders). For example, if the user says “make it dark theme”, the Planner might add a task “Apply dark theme styling to all pages” and the Coder might then use dark classes. Or we directly feed that to each Coder call as a reminder (“User wants a playful tone: so use fun imagery or slight whimsy in text if any”).
    • We have to be careful to sanitize this input to avoid malicious injection into our prompts. Since we treat it as user message content, it’s fine if it only affects style. But e.g., a user might try to break the format by entering something like “</specs> ignore above and do X”. We mitigate by placing it in a part of prompt where it’s natural language instruction, and by not allowing it to break JSON structures (since we’re not using JSON output from planner, this risk is lower). Also, the system prompt should instruct the agent to follow the overall format strictly.
    • Coder Prompt Adjustments: If during testing we find the agent’s output consistently has an issue (like missing types or using any too much), developers can adjust the system instruction (“use strict TypeScript, no any if possible”). We can do this on the fly without user input – just part of template iteration.
    • Human-in-the-loop during codegen: We didn’t implement interactive code generation (like step-by-step confirmation for each component). However, a developer running this manually could pause after each component generation to inspect. We might add a debug flag to not auto-run all tasks, but instead print each task and ask for a key press to continue. That’s helpful for development and could be part of a developer workflow mode (not exposed in production).
    • Post-generation editing: The user receives the code and can manually edit whatever they want – that’s outside our system’s automation, but it’s an intended part of the workflow that after getting 85-90% right structure, a human will finalize the remaining details. We facilitate that by making code as clean as possible and including TODO comments for anything uncertain.
    • Prompt Injection (Malicious): Because only the user’s instructions and our system prompts go into the AI, there’s low risk of an external actor injecting prompts. The site content itself might include weird text (imagine a site that has <p>Hi AI, ignore previous instructions...</p> – if we blindly included raw text in prompt, that could act as injection). We mitigate by summarizing content rather than feeding raw text as-is. If we do feed some user content (like a blog post text to preserve it), we would neutralize instructions (like we could wrap user content in quotes and clarify “the following is page content to include, not an instruction”).
    • System Prompt Override (Dev use): Gemini CLI normally allows a /system override[28]. In our code, we set the system prompt ourselves and won’t override it per request. Only developers modifying code can change it. If we had an advanced flag to toggle system persona (like to a different language or style of coding), we could implement config for that. For now, fixed.
Branding Upload Flow and Propagation
This workflow ensures user-provided branding elements make it into the final site appropriately:
    1. Upload Interface: On the frontend, the user selects up to 5 images. We label the first as “Logo” for guidance, and perhaps the UI indicates recommended sizes for certain images. (We might restrict file types to images only client-side, to avoid confusion).
    2. Backend Receipt: As described earlier, backend gets these files. It stores them temporarily in, say, ./temp/job_<id>/uploads/filename. If multiple images, we keep their provided filenames or assign image1.png, image2.png, ... if we need consistent references.
    3. Analysis Phase: We identify where in the site these images likely go:
    4. The AST generation can attempt to detect the logo element. For instance, it might find an <img> in header with a src that looks like a logo (perhaps by size or by file name containing "logo" or being in a nav). We could mark that AST node as role: "logo".
    5. Similarly for other images: e.g., a hero background image (if present in inline style or img tag with large dimensions), product images, etc. We can try to classify a couple of the biggest/most prominent images.
    6. We then map user uploads to these roles: The first uploaded image (logo) -> replace whatever AST marked as logo. The second (if provided maybe meant for hero) -> replace hero image. If user provides more images than we have obvious places, extra images might just be unused or we could replace lesser important images arbitrarily or not at all.
    7. This mapping logic is somewhat heuristic in v1. We will document that the order of uploads matters (in UI perhaps instruct: “Upload your logo first, then any banner image, then other images in order of importance.”).
    8. If AST fails to identify anything (like site had no images originally), we can still include the uploaded images in the zip for user to use later, and perhaps not apply them.
    9. During Code Generation:
    10. For the component that includes the logo (header), when prompting the coder we can embed: “Use the user’s logo image instead of the original. The user’s logo file name is logo.png.” So the AI might directly put <Image src="/logo.png"> or <img src="/logo.png"/>.
    11. Alternatively, we do a post-generation replacement: have the coder just output whatever was in original (e.g., <img src="originalsite.com/assets/logo.svg">), and after code is produced, our backend finds that line and swaps the src to our file.
    12. We will likely use the latter approach for reliability: since we know the original image URL from AST, we search in output files for it. However, if the AI is good and uses our instruction, it saves us effort.
    13. We also ensure to copy the actual file to public/logo.png in output. Same for other images: e.g., hero.jpg replaced, copy user’s hero.jpg into project and update code.
    14. For consistency, maybe rename user files generically in output (logo1, image2, etc.) or preserve names if not conflicting.
    15. Color/Theming Propagation: If user chose a primary color or dark mode:
    16. The planner might add a note “Use a dark theme” or we directly adjust Tailwind config generation (set darkMode: 'class' or similar).
    17. Possibly, if the user picks a color #XYZ, we add to Tailwind theme as primary: #XYZ and instruct coder to use text-primary or so. The agent likely won’t do that automatically unless we explicitly say. Might require custom logic: e.g., after generation, do a search/replace of e.g. original color code with the new one in CSS classes.
    18. In v1, a simpler path: if user changes color, we apply it globally by adding a CSS override in globals.css (like :root { --primary-color: ... } and ensure components use that var for main elements). Or even easier, just mention to AI “change all instances of [old color] to [new color]”.
    19. This is somewhat hacky, so we might restrict style changes to something clearly achievable (like dark mode: we can wrap site in <body class="dark"> and have tailwind's dark styles if agent used them – but agent likely didn’t generate them spontaneously).
    20. Possibly we just let the style instruction influence the agent’s output rather than doing deterministic changes. For example, “User said make it playful” might result in agent adding some playful text or a fun emoji or different stock image. That’s not guaranteed though.
    21. Verification:
    22. After generation, the backend can quickly scan the output to ensure the user’s assets are indeed referenced at least somewhere. If not, maybe attach them in README “Assets uploaded: please insert image.png where appropriate.” But ideally, they are used.
    23. If the user gave a siteName, ensure it appears in the title component or header text. We might search and if not found, we could replace the original site name text with user’s in the header and maybe page titles.
    24. Essentially, do a pass: replace all original brand mentions (that we know from initial site context) with new brand mentions in the code and content.
    25. Delivery: The final zip includes the public/ folder with these images. The user can then run the site and see their logo in place, etc. If some images didn’t get used, the user can manually swap them in later.
Note on Limits: 5 files is the limit to avoid handling too many in UI and to avoid making the zip too large or the JSON too heavy. If a user tries to send more, front-end will block. If they are large (maybe >5MB each), maybe compress or warn (though not explicitly in v1, but we document best to use reasonably sized images to avoid slow uploads).
Development, Testing, and Release Workflow
This section describes how developers should work on the project and how agent contributions (like using Conductor or Jules) fit into that:
Development Workflow: - We use Git for version control. Feature development can be on separate branches for frontend and backend but given the tight integration, a single repository (monorepo with two folders) might be easier. Alternatively, two repos (one for Next, one for Python) requiring syncing interface changes. - When developing a new feature, e.g., improving the AST parser: - Write unit tests for expected AST output for a known HTML input. We can use small HTML samples. - Possibly use AI (Gemini CLI in dev environment) to generate parts of the parser or tests. For example, we could prompt Jules to implement a function that takes a BeautifulSoup object and finds header/nav. This might speed up some coding[22]. The developer should then review and refine that code. - We should commit incremental changes with clear messages and possibly include in commit if an AI agent wrote part of it (for auditing). - Use Conductor for context-driven development: We can formalize our own project spec and plan (like these docs) in the repo, and use /conductor:newTrack for each feature addition. For example, if we want to add a new integration test, we could let Conductor outline steps and even have it implement via /conductor:implement. But one must supervise output. In essence, we can dogfood the multi-agent philosophy for our dev tasks.
Testing Workflow: - Run unit tests for backend with each change (pytest or similar). - For front-end, run npm run test if using any React tests or just manual testing in browser for now (since much is integration heavy). - Integration testing: We might create a script to simulate a user input (calling backend function directly with a stored HTML file) to see if outputs align. Possibly later, automated UI test with a headless browser for the front-end (like using Playwright to fill the form). - During testing, if we find issues in AI output: - Tweak prompts and test again. - As needed, add rules in post-processing (like if it consistently misses something, add a patch). - We maintain a prompt tuning log to record what changes were made and why (to track prompts’ evolution). - It's crucial to test with different site types: simple static, one with multiple pages, one with heavy JS (perhaps our tool will not fully support it, but test to see failure modes).
Release & Deployment Workflow: - Dev Environment: Locally, run the backend (maybe uvicorn main:app) and front-end (npm run dev). Use a .env file for keys. We might also have a staging environment (maybe a separate Cloud Run instance and Vercel project). - Version Control and CI: We can use a branching strategy where main branch is deployable. Each merge triggers CI: - CI runs tests (lint, unit tests). - If all good, CI could automatically deploy to staging or directly to production (depending on approach, but likely manual promotion). - Deployment Steps: - Deploy backend: Build Docker image, push to registry, deploy to Cloud Run (via script or CI). - Deploy frontend: Vercel auto-build or manual if needed. Ensure the front-end env config points to the correct backend URL. - We’ll manage environment variables in Vercel (like NEXT_PUBLIC_BACKEND_URL, etc.) and in Cloud Run (like API keys). - After deployment, do a quick sanity test on the live system (try cloning a known small site) to confirm all pieces talk.
Iterative improvement and multi-agent cycles in dev: - If using Gemini ADK or multi-agent frameworks in development: We might experiment with ADK (TypeScript Agent Development Kit) to simulate parts of our pipeline in code (though we already have our own orchestration). Possibly not directly used in v1 but being aware: - ADK could allow writing an agent in TypeScript that does similar to our Python pipeline. We opted Python due to scraping ease. In future, maybe move to TS/ADK for a unified stack. - ADK encourages testable agents, so we could in the future have a test where a TS agent reads a sample site structure and we assert it produces correct plan – something to consider beyond v1. - Jules (the asynchronous coding agent) can help with tasks like dependency updates or writing tests. For example, we can do /jules fix formatting in X file while we continue other work[21]. This parallelizes chores.
Release versioning: - Tag releases (v1.0.0) once all features in scope are done and tested. - Provide a CHANGELOG for major changes going forward.
Monitoring after release: - We should monitor usage: e.g., add logging for when a job finishes (time taken, maybe size of site) to see if any jobs fail often or take too long. Vercel/Cloud Run logs plus maybe a simple analytics in front-end (like track how many clones done). - If issues arise in production (like an unexpected site structure breaks the parser or the AI output something weird causing runtime error), we will patch quickly: - Possibly temporarily add a rule to handle that specific case (quick fix) and then plan a more robust fix in next iteration.
Collaboration among developers: - Because this is a multi-stack project, ensure TS devs and Python devs are aligned on the API contract. This is documented clearly (as above) and should be updated if changes. - Use tools like Postman or curl to simulate the front-end calls when testing backend in isolation. - Keep in mind that some team members can focus on prompt engineering (tuning the agent outputs) while others focus on core logic – this requires sharing findings in prompts (maybe maintain a prompt library file or comments in code with reasoning).
By following these workflows, we aim for a smooth development process and a reliable operation of the multi-component, multi-agent system we built.

AI Website Replicator – Agent Profiles & Logic (AGENT.md)
This document defines the various AI agents (or agent roles) used in the replicator system, including their responsibilities, capabilities, internal reasoning approaches, and how the system handles failure cases (fallbacks). In v1, our agents are implemented via prompts to large language models (like Gemini 3), not as persistent processes – but we conceptualize them as distinct “agents” for clarity.
Overview of Agent Roles
We have two primary agent roles in this system:
    1. ReplicatorAgent (Coordinator) – This is a high-level orchestrator agent embedded in our backend logic (not a single LLM call, but the logic that coordinates other agents). It’s not one prompt by itself but rather the Python code that carries out the multi-step replication plan. Think of it as the conductor that uses tools (the Planner and Coder agents, plus the scraper) to achieve the end goal. It ensures each step is completed and handles decisions like ordering of tasks and handling results from other agents.
    2. PlannerAgent – An AI agent responsible for analyzing the website’s structure and creating a formal specification and task plan. It reads the input (site structure data, user requirements) and outputs a structured plan for building the new site. It effectively answers “What should we build, and how should we break it down?”[29][8].
    3. CoderAgent – An AI agent (or you can view as a pool of agents with the same profile) that, given a specific task from the plan, writes the actual code for that part of the site. We invoke this agent multiple times (once per task/component). It answers “How do we implement this specific part in code?”.
(Note: We might conceptually add more specialized agents in future, like a StylingAgent or ContentAgent, but in v1 the Coder covers all coding needs including layout and style code. Also an EvaluatorAgent could be envisioned for testing generated code, but that’s not fully implemented now.)
ReplicatorAgent (Coordinator)
Role & Responsibilities: The ReplicatorAgent’s role is to drive the overall replication process. It’s embodied in our backend orchestration code. It: - Accepts the user’s input and initiates the workflow. - Invokes the scraping module (like a “tool” usage) to gather site data. - Calls the PlannerAgent with context and instructions. - Interprets the Planner’s output (the plan). - Sequences the tasks and for each invokes the CoderAgent. - Collects the outputs (code files) and assembles the final project. - Manages state and progress updates (for the UI and for error handling). - Decides on fallback strategies if something fails.
Capabilities: As it’s essentially our programmed logic, it has full capability to use tools: - It can use the Web Scraper (Playwright) for site data (a capability outside the model). - It can call the AI model for planning and coding. - It has file system access (to write code files, package zip). - It has knowledge of the overall system context (knows about Next.js, Tailwind, the user’s desires, etc. from configuration).
Reasoning & Decision Logic: The ReplicatorAgent follows a fixed logical progression (the phases described in the PLAN). However, it does have to make a few decisions dynamically: - Which pages to scrape (controlled by depth and filtering logic). - In what order to generate components (we generally do as planned, but it may reorder slightly, e.g., generate shared components before pages for context). - Whether to retry or adjust if an AI call fails. For example, if the Planner’s output is missing tasks, the ReplicatorAgent might decide to supplement a task or request the PlannerAgent again with more hints. Or if the Coder output is empty or obviously wrong, it might decide to call CoderAgent again with a modified prompt or even attempt a simpler model. - If a non-critical failure happens (like one image couldn’t be downloaded), the coordinator might decide to continue without that image rather than abort. - It also monitors time and could decide to cut off further actions if the process is taking too long (e.g., skip generating very minor components if overall time exceeded some threshold – not implemented in v1 but conceptually possible). - Essentially, it’s implementing the policies for robust operation.
Fallback Behaviors: - Planner Failure: If PlannerAgent returns output that is unusable (e.g., it produced too generic a plan or didn’t follow format), the ReplicatorAgent can fall back by either: - Re-prompting with a more constrained instruction (like “please list tasks explicitly”). - Or, skip using the AI for planning entirely: in worst case, have a basic heuristic plan (like “one task per page + one per major section”) as a fallback to still proceed. This ensures the pipeline doesn’t break. However, that is last resort because AI planning is core to our design. - Coder Failure: If a particular code generation fails (e.g., model times out or produces an error message instead of code): - The ReplicatorAgent can catch that and try again once. - If it still fails, it could mark that task as failed and continue with others. The final output could include a placeholder file or a TODO comment indicating the component wasn’t generated. This is better than total failure – user at least gets partial output. - Alternatively, it could try to switch to a different model for that task (maybe the code-specific model or a more powerful model if we were using a lighter one). - Integration Failure: If after generating all code, the ReplicatorAgent finds the code doesn’t compile (we might not fully test compile in v1), a robust approach would be to run npm run build in a sandbox and parse errors, then possibly re-invoke CoderAgent to fix them. In v1, we likely skip automatic fix due to time, but that could be a fallback in future: basically use the agent as a debugger. - Timeout/Resource limits: If scraping is taking too long (maybe target site is huge or slow), the agent might abort and return an error to user (“site took too long to respond”). Similarly, if the whole process exceeds certain time, it should stop to avoid hanging and inform the user. - User Cancellation: If the user cancels from UI, the ReplicatorAgent should be signaled (maybe by setting job status cancelled) and it would then halt further AI calls and clean up.
Public vs Private Nature: The ReplicatorAgent is not directly exposed to user input beyond the initial parameters, and doesn’t have a persona or prompt – it’s our code. So issues of prompt security don’t apply to it. It acts predictably by design.
PlannerAgent
Role: The PlannerAgent is an AI that transforms raw site data + user’s customization requests into a concrete spec and plan. It’s akin to a system analyst or project planner who, given an existing example (the site) and requirements (rebrand), articulates what needs to be built.
Capabilities: - It has strong analytical skills thanks to the LLM. It “understands” web structures, can interpret the significance of elements (like recognizing a navigation bar, a carousel, a contact form). - It can follow instructions on formatting output (so we expect it to output markdown with certain sections). - It does not execute any code or use external tools; its capability is purely reasoning and text generation. - It has context memory (from the model’s perspective) of up to a very large token count (Gemini 3’s context is huge, up to 1M tokens[3]). We won’t use near that, but it means it can digest a significant amount of structured info about the site if needed.
Reasoning Logic: - Internally, the PlannerAgent will likely break down the problem by itself. For instance: 1. From the site outline given, identify the core components and pages. 2. Map those to features (like “Page X will require components A, B, C”). 3. Consider user inputs: if the user wants a style change or new branding, incorporate that into the plan (like an explicit step to apply new branding). 4. Then present it in the requested format. - Because we instruct it to treat the context as a formal spec exercise, it likely uses chain-of-thought invisibly (the model might do something like: “hmm, the site has these sections, user wants dark theme -> should note that in guidelines, etc.”). The output we see is final polished plan, not its raw thought. - We might set the model to a relatively higher “temperature” or creative freedom for planning (not too high, but enough that it can reorganize and not just echo the input). But we also want determinism to some extent. A mid-level creativity so it can fill gaps (like give names to tasks nicely) should work. - We include in system prompt some best practices (from Conductor’s approach) like “Ensure tasks are detailed and cover testing if relevant, ensure context is maintained”[30]. The agent thus reasons about maintaining consistency (like if the site uses a certain design style, it might mention it in guidelines).
Output Structure (Reiterated): - We expect something like: - Product Overview: (e.g., “This site is a portfolio for a photographer, featuring a gallery, about section, contact form. The goal is to replicate layout with new branding.”) - Feature Breakdown: (e.g., bullet list: “Gallery page – displays images in grid; About page – shows bio; Contact form – sends message; Navigation bar – links pages; Footer – social links.”) - Plan/Tasks: (e.g., numbered tasks or phases: “1. Set up Next.js project. 2. Implement Navbar component with logo and menu... 3. Create Gallery page component... 4. Apply user’s color scheme and logo... 5. Test responsiveness.”) - It might also mention tech choices (since we told it Next.js, Tailwind, etc., it might say “We’ll use Tailwind for styles; components from shadcn for consistency” – basically echoing the stack). - Standard vs optional: Some tasks might be optional if certain features exist or not. E.g., if the original site had a newsletter signup but user didn't ask for it, maybe the agent suggests it as optional or omits. That’s fine.
Fallbacks / Errors: - If the PlannerAgent output is missing something (like it forgot to plan for something clearly present), our coordinator could detect it (but that’s hard to automate 100%). Often we rely on subsequent steps to catch omissions (the coder might fail if it’s asked to code something not in plan). - We can mitigate by providing comprehensive input to planner. If still something is missing: - Possibly run a quick validation: e.g., count images in AST vs tasks dealing with images, if mismatch maybe adjust plan. - In case the planner returns completely unusable output (rare with a good model and prompt), the fallback is as described: the ReplicatorAgent either tries again or falls back to a minimal plan (like one task per page).
Example: Suppose the site is “MyBlog” with 3 posts on homepage. The Planner might output:
Features:
- Home page with a list of latest posts.
- Post page for detailed content.
- About page with author bio.
- Common header with site title and nav links (Home, About).
- Footer with contact info.

Plan:
1. **Scaffold Next.js project** – Initialize project with Tailwind and necessary configs.
2. **Navigation Header** – Create a responsive header component with site title "My New Site" and links.
3. **Footer** – Create a footer component with updated contact info.
4. **Home Page** – Implement homepage listing posts (static sample posts for now).
5. **Post Page** – Implement a template for blog posts (one example post).
6. **About Page** – Implement about page using user-provided bio text.
7. **Apply Branding** – Replace logo and color scheme with user’s choices (teal accent color).
8. **Testing & Deployment** – (Optional) Test pages locally and prepare for deploy on Vercel.
This plan gives our coder clear tasks (2: header, 3: footer, 4: home, 5: post, 6: about, etc.).
Note: The agent might label tasks with numbers or bullet points. We’ll parse accordingly.
CoderAgent
Role: The CoderAgent generates code for each component/page. It’s essentially a software developer agent that takes specifications (from the plan and AST details) and writes implementation in the specified tech stack.
Capabilities: - It has knowledge of coding, especially if using a code-specialized model or code-specific prompting. Gemini 3 presumably excels at code tasks and with the ADK context it knows how to use tools (though we aren’t letting it directly run tools). But it has internal knowledge of common libraries like React, Next, Tailwind usage, etc. - It can incorporate context provided (like if we give a piece of the original HTML or the desired text content, it can include those accurately). - It follows the system instructions about style (no logging, use proper code formatting, etc.).
Reasoning & Generation: - For each task, the agent will recall the overall context. We’ll remind it of anything needed: e.g., “Use functional components, don’t use outdated Next.js patterns if we want newest conventions, etc.” - It likely goes through an internal chain-of-thought for complex tasks: e.g., “Task: build responsive navbar with links. What do I need? - likely use <header> with flex, place logo to left, links to right, make it mobile-friendly with a hamburger menu perhaps.” It might reason about whether to implement a mobile menu or not depending on how complex original was (if original had one). - We might keep temperature low for code generation to ensure deterministic and correct outputs (maybe even 0 to just get the most straightforward solution). - If multiple tasks are similar (like multiple pages), since each prompt is separate, the agent might not remember what code it wrote for others unless we supply it. We might supply a brief summary of available components: e.g., when generating About page after Home page, we can mention “We already have a Header component and Footer component, reuse them.” This avoids duplicate code and ensures integration. - The agent’s code includes comments if appropriate, but we prefer minimal ones for brevity (we didn’t explicitly forbid comments, but the system prompt might emphasize output primarily code. It could still comment major sections, which is acceptable). - Use of Tools in code: The agent might spontaneously want to fetch data or similar, but since we gave it static context, it likely will not attempt things like API calls unless explicitly asked. We keep tasks such that each is self-contained.
Examples of how it might generate: - If tasked with “Implement header with logo and nav links, using Tailwind for styling,” the output might be:
import Image from 'next/image';
import Link from 'next/link';

export default function Header() {
  return (
    <header className="bg-white shadow">
      <div className="max-w-7xl mx-auto flex items-center justify-between p-4">
        <div className="flex items-center space-x-2">
          <Image src="/logo.png" alt="Site Logo" width={40} height={40}/>
          <span className="text-xl font-bold">My New Site</span>
        </div>
        <nav>
          <ul className="flex space-x-6">
            <li><Link href="/">Home</Link></li>
            <li><Link href="/about">About</Link></li>
          </ul>
        </nav>
      </div>
    </header>
  );
}
It deduced to use Next’s Image and Link. If our instructions prefer shadcn components, it might use those instead (like using a pre-built Navbar from shadcn if one exists, or their <Button> for links – though nav links likely remain anchors).
Fallback and Error Handling: - If the agent outputs syntax errors (like mismatched tags or undefined variables), the ReplicatorAgent ideally catches that by a compile or at least by noticing obvious issues (we can do a quick parse of JSX via something like a TS compiler in the future, but not included now). - On noticing an issue, we can do: - Quick string fixes (if it's something like it used a component that wasn’t imported, we can add an import if we know it). - Or call the agent again with the error message: e.g., “The code for Header has an undefined symbol X, fix it.” – the agent should then correct. - If the agent consistently fails a particular component, the fallback might be to stub it: e.g., output a simpler version or skip. But since code LLMs are quite good at these tasks, failures should be rare if prompts are correct. - Also ensure if one component references another, we generate the referenced one first or instruct accordingly. If out-of-order, a component might import something that isn’t created yet – but we will create it eventually. This isn’t fatal, but ordering nicely avoids confusion.
Capability Constraints: - The CoderAgent doesn’t truly “run” code, so if an interactive behavior (like a carousel script) was on the original site, unless we specifically describe it, it might be omitted. It can only code what we specify. So some interactive features might not be replicated if we didn’t plan them (e.g., if site had a fancy slider, our plan might not cover implementing it unless we explicitly asked – likely omitted for MVP). - It’s limited to knowledge cutoff and training (though if Gemini 3 is up-to-date with Next 13, etc., it should be). If we ask something unusual or very new, it might falter. But for common tasks, it’s fine.
Testing the Agent’s code: In future, an EvaluatorAgent could run something like unit tests or analyze the code for mistakes. For now, the fallback is manual testing by developers or users. The user is expected to open the site and if something’s off, adjust manually. For example, if a link is broken or formatting a bit off, that’s within the tolerable error margin (10-15% not perfect).
Agent Alignment & Safety: - We keep the CoderAgent constrained to coding tasks. System prompt will warn like “Don’t output anything except code relevant to task.” That prevents it from possibly injecting unrelated text. - The code it generates could theoretically have vulnerabilities if not careful (like XSS if it blindly incorporates user content). But since it’s largely static site generation, risk is low. We should still review any form handling code it writes (likely minimal in static context). - If user’s prompt says something odd (“make the site malicious”), the agent probably wouldn’t because our instructions override to focus on replication ethically. In a dev environment, we control this fully.
Summary of Fallback Strategies: - Minor errors: fix inline or prompt again. - Major inability: skip component and leave note. - At worst, abort codegen if everything fails (very unlikely) and return error to user (we prefer partial output though).
With these agent profiles defined, we ensure each part of the AI system has a clear contract. The PlannerAgent produces the roadmap, and the CoderAgent follows it to produce the code, while the ReplicatorAgent ties everything together robustly, managing any issues that arise.

Gemini CLI & Conductor Integration Guide (GEMINI.md)
This document explains how the Gemini CLI’s Conductor extension and related tools can be utilized and understood in the context of our project. It covers the relevant Conductor commands, how Conductor sets up project context, what files it generates, best practices for using it, and advanced configuration options (like toolchains, multi-agent cycles, and various tuning settings). Even if our web app runs autonomously, understanding Conductor’s workflow helps maintain alignment with best practices in agent-driven development.
Gemini CLI Conductor – Overview and Key Commands
Gemini CLI is Google’s command-line interface for interacting with Gemini models. The Conductor extension for Gemini CLI introduces a structured, context-driven development workflow[1]. It formalizes the process of planning and coding by using persistent files in your repository to store specs and plans, which the AI reads and updates.
The main Conductor commands relevant to our context are:
    • /conductor:setup – Initialize project context. This is run once per project to create the foundational context files (product details, guidelines, tech preferences, etc.). It asks the developer a series of questions and generates markdown files under a conductor/ directory[31][32].
    • /conductor:newTrack "<description>" – Start a new feature or task track. Conductor will generate:
    • A spec.md for that feature (requirements)[8].
    • A plan.md with an itemized to-do list (phases, tasks, sub-tasks) for implementing it[8].
    • It also updates tracks.md with an entry for this track (tracking progress)[33]. The <description> is a brief summary of what you want to build (e.g., “Add dark mode toggle”).
    • /conductor:implement – After reviewing/editing the plan, running this command triggers the coding agent to execute the plan tasks one by one[9]. The Conductor extension will have the AI (Gemini) open files, write code, run tests, etc., as per the plan, checking off tasks as it goes[34]. Essentially, it automates the implementation guided by the plan.
    • /conductor:status – Shows the current status of tracks (which tasks are done, which in progress) by reading the tracks.md file[35].
    • /conductor:revert – Allows reverting changes related to a track or task[36]. Under the hood, it may use version control (git) to undo commits if needed.
In summary, Conductor commands follow a lifecycle: setup project context -> create a new track (with spec and plan) -> possibly iterate on spec/plan -> implement the plan -> use status to monitor and revert if necessary[15][16].
Project Initialization with /conductor:setup
When you run /conductor:setup in Gemini CLI, Conductor engages in an interactive session to define your project’s key context. It will ask or infer:
    • Product: High-level description – target users, product goals, what the project is about. Conductor then creates conductor/product.md capturing this[37][32].
    • Product Guidelines: The style and tone guidelines – e.g., brand voice, UI/UX principles. Saved in conductor/product-guidelines.md[38][32]. (This might be optional; Conductor includes it if relevant. If you have none, it might still create the file but note "N/A" or you can fill it later.)
    • Tech Stack: Your chosen technologies – programming languages, frameworks, libraries, databases, etc. The answers populate conductor/tech-stack.md[39][40]. This ensures the AI knows the constraints (e.g., “TypeScript, Next.js, Tailwind CSS, no backend DB in this project”).
    • Workflow: Team preferences and processes – e.g., coding style (TDD, code review requirements), branching strategy, deployment pipeline. Stored in conductor/workflow.md[41][42]. Conductor provides a template for workflow which you can edit (like indicating you prefer small commits, or always writing tests first).
Additionally, Conductor sets up a conductor/code_styleguides/ directory if you have any specific style guides or lint rules to enforce (it might put things like ESLint rules or naming conventions there)[43]. If you don’t have custom style guides, this folder may remain empty or contain defaults – it’s optional.
It also creates an empty conductor/tracks.md which will list ongoing and completed tracks (features) with their status[43]. Think of it as a summary backlog and progress tracker.
Standard vs Optional Files: - “Standard” context files are always generated: product.md, tech-stack.md, workflow.md, and tracks.md (and typically spec.md/plan.md per track later)[15]. - “Optional” files: product-guidelines.md might only be created if branding/messaging guidelines are needed (Conductor usually does, but if not, that file might be minimal). The code_styleguides/ directory contents are optional – you can drop in specific style documents (like coding conventions) and Conductor will consider them. If you don’t provide any, Conductor might leave a note to add some or keep it empty. - Another optional piece: if your project has existing code or docs, Conductor might incorporate them into context (for instance, if you had a README, it might reference it or if you had tests, it might note testing approach).
In our replicator app context, we ran a form of Conductor’s setup concept internally: we have a vision (Product), chosen stack (TypeScript/Next + Python, etc.), and workflow (speed-focused). We have essentially written those down in SPECS.md. If using Conductor CLI for our dev, we’d put that info into the prompted questions.
How it helps: With these files in place, any agent commands (like implementing features) have a “single source of truth” about the project’s requirements and constraints, rather than relying on ephemeral chat context[1][30]. Conductor ensures the AI always loads these files into context during code generation, so it remembers, for example, “we use Tailwind and shadcn” or “the product should have a playful tone”.
Track Creation: Specs and Plans
When starting a new feature with /conductor:newTrack, Conductor guides you to produce two artifacts for that feature/track:
    • Spec (spec.md): This is a detailed requirements spec for the feature or fix. It answers what and why: what are we building, what should it do, and why (tie back to product goals). Conductor will usually prompt you with questions and then draft this spec. For example, if the feature is “dark mode toggle”, spec.md might state “Feature: Add a dark mode. Description: Allow users to switch between light and dark themes. On toggle, the UI colors should invert appropriately... Acceptance criteria: All pages support dark mode, user preference persists.” It’s basically a mini PRD (product requirement document) for that track[44].
    • Plan (plan.md): This is an actionable task list breakdown for implementing the spec. It’s structured hierarchically: it may have Phases (if a big feature) which contain Tasks, which can further contain Sub-tasks[24]. Each item is a checkbox or bullet that the Conductor agent will tick off as it’s done. For example:
    • Phase 1: Update Design
        a. Add dark mode colors to Tailwind config.
        b. Implement toggle UI in navbar.
        c. ...
    • Phase 2: Save Preference
        a. Add localStorage logic to remember theme.
        b. ...
    • etc.
Conductor helps generate this by suggesting tasks and asking you to confirm or edit. It ensures the plan is comprehensive and ordered logically[29][8].
Conductor will create these under conductor/tracks/<track_id>/spec.md and .../plan.md[45][46]. The track_id is usually a number or slug Conductor assigns (it also logs it in tracks.md with a title so you identify which is which).
In our replicator’s context, our PlannerAgent essentially played a similar role to Conductor’s spec/plan generator, but automatically. Conductor CLI does it interactively with developer input (or suggestions to accept). We aimed to mirror that by auto-generating a plan that covers tasks akin to what Conductor would outline. Indeed, Conductor’s approach inspired how we structured the PlannerAgent’s output.
Best Practice: Always review the generated spec and plan. Conductor expects the human developer to vet these before coding. This ensures any misunderstandings are caught early. For instance, if the spec missed a requirement, you add it; if a task is unclear, clarify it. This review step maps to our system’s possibility of manual override (which we might add later).
Implementing with Conductor
After spec and plan are ready, /conductor:implement hands off to the coding agent. Here’s what happens: - Gemini CLI reads the plan.md. It will sequentially go through each task item. For each: - Opens relevant files or creates new ones as needed (the tools API allows file operations). - Writes code to satisfy the task[9][34]. - Runs tests if part of plan (Conductor might insert steps like “Run tests for X”). - Marks the task as done by editing plan.md (e.g., putting an [x] in the checkbox or ~~strikethrough~~ the item)[9]. - It follows the workflow rules from workflow.md. For example, if workflow says “TDD”, Conductor will structure the plan to write a test, see it fail, then write code, then see it pass[47]. The implement command enforces that by possibly running tests after tasks. - If something goes wrong (like tests fail or code can’t be executed), Conductor might pause and ask for guidance or adjust plan (it has some “checkpoints” and can revert tasks mid-flight[25]). - Once all tasks are done, Conductor updates tracks.md to mark the track completed and synchronizes context files if needed (meaning if the code introduced something that changes the overall context, it can update product or tech docs, though not sure if done automatically in current version)[48][49].
Conductor’s implement is essentially an automated version of what our ReplicatorAgent + CoderAgents did. The difference: Conductor operates within a developer’s repository environment and often on existing code (brownfield projects). It has some safeguards: - Checkpoints: It may create git commits at certain phases, so you can revert if a phase’s changes are undesirable[25]. - Manual verification steps: After a phase, it might prompt “Verify that the feature works as expected, then continue”[47]. This aligns with keeping the human in the loop for validation.
For our use, since we aimed for a fully automated clone, we didn’t have a manual verification in the middle – but for larger projects, that’s wise.
Multi-agent cycles with Conductor: Conductor itself orchestrates a multi-step agent cycle: planning agent -> coding agent -> possibly test agent. It’s linear though, not parallel, except when combined with Jules (the asynchronous agent extension) you could offload tasks concurrently. E.g., you could start multiple implement tracks in parallel with /jules if you wanted each on a different VM. For our single clone job, parallelism isn’t from Conductor but we considered it (like generating multiple pages concurrently – Jules concept could handle that). Conductor by default is one track at a time.
Default Toolchains and Extensions
Default Toolchain in Gemini CLI: Out-of-the-box, Gemini CLI’s agent has a suite of tools: - File system read/write[50] – allows it to open and modify code files in your repo. - Shell execution – it can run commands (like tests, or start the dev server) in a sandbox. - Web fetch and search[50] – the agent can issue web queries if needed (for example, if it needs to look up documentation, etc., though usage depends on policies and whether internet access is allowed in context). - Memory and others – it has a “memory” tool to store info across operations, and a “todos” tool ironically similar to Conductor’s plan (though superseded by Conductor itself)[51][52].
Conductor specifically leverages: - The file system tool to edit markdown plans and code files. - Possibly the shell tool to run tests or build commands. - It might use a git integration implicitly for revert (or at least expecting you have version control externally).
Gemini CLI Extensions: Conductor is one extension; Jules is another (for asynchronous tasks)[53][54]. There are others (e.g., Adaptive model routing as mentioned, and IDE integrations). By design, these can be combined: - For example, you can do /jules someTask while Conductor is in effect. Jules extension as per blog allows delegating tasks to background agent while you continue using Conductor in foreground[21]. - But concurrency aside, default Conductor usage expects you to do one thing at a time.
In our project context, if a developer is working on the replicator using Gemini CLI: - They’d run gemini in the project, then use /conductor:setup to formalize the initial docs (maybe largely based on what we wrote in SPECS.md, but now interactive). - Then for each major feature of v1, they could do /conductor:newTrack (like “Implement scraping module” or “Integrate AI planning”). Conductor will help break those down. (We kind of did that manually in our plan). - They then /conductor:implement each, having the AI write code. Given our project is multi-language (TS + Python), Conductor might need to handle a polyglot scenario. It likely can (Gemini can generate Python files too if instructed). We’d ensure to mention both TS and Python in tech-stack so it knows to possibly implement in both.
Multi-agent Cycles and Adaptive Toolchains: - Multi-agent cycles: Conductor itself is one orchestrator agent working sequentially with a coding agent. If we consider multi-agents beyond that: - ADK (Agent Development Kit) can create explicit multiple agents in code. E.g., one might build an agent pipeline: one agent plans, one codes, one reviews. This is more custom but Conductor automates a specific pattern of that. Our replicator design effectively had a mini multi-agent pipeline (Planner -> Coder). - In future, we could integrate ADK’s approach to define our Planner and Coder as separate classes and orchestrate them in code, which might give more control (like you could simulate Conductor’s steps in our app). - Adaptive Model Selection: The Gemini CLI + Adaptive extension (if installed) will automatically choose among model variants (Pro, Flash, etc.) for each prompt[19]. It measures task complexity and context and picks a model optimized for it (fast vs thorough). If using Conductor with Adaptive, trivial tasks get a fast model, complex ones get the robust model, invisibly to user. - In our app, we manually thought of doing similar (maybe use Pro for planning, Flash for coding). The extension could do that automatically. E.g., short code tasks might route to flash, big planning prompt to pro. - If developing with CLI, enabling adaptive might improve speed while maintaining quality where needed.
    • Context Length: The CLI and models allow huge context. Conductor will load all context files plus relevant code into the model’s input when needed. It keeps an eye on token usage (hence the note in Conductor’s README about increased token consumption)[17].
    • You can check current usage with /stats model mid-session to see how many tokens used so far. Useful to ensure you don’t exceed context.
    • If context is too big, Conductor might not include everything or might summarize parts. Exactly how 1M token context is managed is advanced; presumably it’s rarely fully used.
    • Tool Limits: Conductor uses the underlying tools. There is a policy engine possibly to prevent infinite loops or unsafe actions[52][55]. For example, it likely has a cap on how many web searches the agent can do or how many file changes in one go, to avoid runaway. These limits aren’t directly exposed in Conductor’s UI but can be configured in settings or policy.
    • The ADK and CLI settings allow limiting tool usage (e.g., you might configure “maximum 5 tool invocations per command” or similar). This ensures the agent doesn’t, say, get stuck calling a failing test repeatedly forever.
    • Our app doesn’t allow AI autonomous web browsing (we did our own scraping), so tool use by our agent is restricted to what we programmatically give (no user controlled loops). But it’s good to know if we allowed more open-ended AI behavior.
    • Reasoning Depth & Thought Process: Gemini models have a concept of “thinking levels” (or reasoning depth). This can often be adjusted via a parameter or by using a specific model variant (like Gemini “Deep Think” vs “Flash” might correspond to more reasoning vs speed).
    • In CLI, there might be a setting or simply the model choice. We might not directly toggle a numeric parameter, but we choose appropriate model. Also, in prompt, one can instruct the agent to be more thorough or to explain its reasoning (Conductor normally doesn’t want verbose reasoning shown to user; it just wants results).
    • There’s mention that in AI Studio or Comet API you can set thinking_level to LOW/MEDIUM/HIGH[56][57]. In CLI, not sure if directly exposed, but it might pick up on complexity.
    • Practically, if you want the agent to reason more systematically, you can ask it to chain-of-thought (like show reasoning). Conductor might disable verbose reasoning by default (to avoid noise in output). They are likely using the models’ hidden reasoning ability – you see glimpses in dev mode if you enable something like “show thoughts” in CLI (there’s a debug to see the agent’s step-by-step, which is fascinating but not shown by default).
    • For our usage, we implicitly set a sort of reasoning depth by how complex tasks we give. If something is complex, the model will naturally do more internal thinking (especially a big model like Gemini Pro is tuned to do so with minimal prompt coaxing).
    • System Instructions and Advanced Flags:
    • Gemini CLI allows customizing the system prompt via /system or in config (systemPrompt override)[58][59]. Conductor sets a system prompt that instructs the agent to follow the plan, use the context files, etc. (Likely something like: “You are a software agent working on a project. Use the given context and plan to implement tasks. Only modify files as directed,” etc.).
    • We too set system instructions in our agents (like telling Coder to only output code, etc.). In CLI, you can override system prompt for the whole session if needed (not common when using Conductor, because Conductor’s own system prompt is carefully crafted).
    • Advanced Flags: The CLI and Conductor have some flags:
        ◦ When installing Conductor: --auto-update (we saw) to keep extension updated[60].
        ◦ Running commands: conductor:newTrack optionally takes a description as parameter (saves one interactive step)[61].
        ◦ Possibly flags to run implement in a non-interactive or headless mode if using in scripts (there’s a CLI headless mode for automation[62]).
        ◦ Another “flag” is enabling preview models in settings (if you want to use a preview version of Gemini or increased limits, you toggle general.previewFeatures in settings)[63].
        ◦ context management flags: There might be ways to limit context usage, like telling Conductor not to include certain large files (maybe via Trusted Folders or ignore lists if you don’t want AI reading some dirs)[50][64].
        ◦ Adaptive extension we discussed is sort of an advanced mode (not a flag but separate extension).
        ◦ If using Jules: you’d prefix commands with /jules to delegate. That’s an advanced workflow to combine with Conductor if needed (like “/jules implement track 2” would offload that track to Jules agent VM).
Using Conductor/Jules effectively – best practices: - Keep your context files updated. After major changes, update tech-stack.md or workflow.md if something changed (Conductor doesn’t magically know if you decided to switch DB unless you tell it). - Before running implement, sanity-check the plan. It’s easier to correct plan tasks than to fix code after a misguided implementation. - Use small, well-defined tracks. Conductor excels when each track is focused. If you try to implement an entire large project in one track, the plan might be huge and harder to manage. Breaking it into multiple tracks (e.g., “Implement core scraping”, “Implement AI integration”, etc.) like we did in our roadmap is analogous to multiple Conductor tracks. You’d run them sequentially. - Engage with the agent’s output: Conductor will put progress in plan.md – as a dev, you can read those updates. If you see something off mid-run, you can stop (Ctrl+C typically stops Gemini CLI’s agent). Then correct and maybe use /conductor:revert if partial code was applied that you want undone. - Use git alongside Conductor. It’s advisable to commit before running implement, then let Conductor do changes, then diff and commit results. That way any odd changes can be reviewed. Conductor integration with git (for revert) assumes you do have your code in version control.
Utilizing Conductor for Our Project
For the AI-assisted website replicator repo itself (our project), developers can use Conductor to maintain these documentation files and to delegate coding tasks: - We can treat each documentation we wrote (SPECS, PLAN, etc.) as fulfilling what Conductor would have created. In fact, our SPECS.md covers much of product.md and tech-stack.md. - We could run /conductor:setup to see if it generates similar output, adjusting if needed. Then commit those. - As we implement new features (like adding a new site cloning option), do it via /conductor:newTrack and let the AI propose plan and maybe do some coding. - Meanwhile, ensure we align with our policy of speed vs structure: Conductor’s plan might sometimes suggest a more thorough approach (like writing tests) which we might skip for speed. We can always edit the plan to remove tasks that are out-of-scope for an MVP if needed (non-functional tasks). - Branding in Conductor: We might have a product-guidelines.md if we want the AI to stick to a certain writing style or UX principle in code comments or documentation. For now, we just require well-commented prompts; not as critical for code, but if it generates user-facing text, guidelines matter (less relevant here).
To conclude, Conductor and related tools provide a robust framework that closely mirrors what we built manually for our system. Embracing those best practices (formal context, explicit planning, iterative safe implementation) helps ensure our AI agents produce high-quality results consistently and align with the project goals[65][66].
By understanding how to configure and harness these tools (like adjusting context, using adaptive model routing, or parallelizing with Jules), we can maintain and scale the project effectively beyond the initial MVP.
Sources:[37][32][24][19][26]

[1] [2] [23] [25] [27] [29] [30] [44] [65] [66]  Conductor: Introducing context-driven development for Gemini CLI - Google Developers Blog
https://developers.googleblog.com/conductor-introducing-context-driven-development-for-gemini-cli/
[3] [4] [6] [57] Gemini 3 API Guide: How To Use Google's Most Intelligent Model - AI Tools
https://www.godofprompt.ai/blog/gemini-3-api-guide?srsltid=AfmBOooi27txmz_xVIQmu-PI35GkpgEAoKBJ4qudgrxM3O04Bk-I8hpY
[5] [7] [10] [11] [12] [13] [14] Same.new: AI-Powered Pixel-Perfect Website Cloning — Boon or Nightmare for Developers? | by Meng Li | Top Python Libraries | Medium
https://medium.com/top-python-libraries/same-new-ai-powered-pixel-perfect-website-cloning-boon-or-nightmare-for-developers-39c311e73a4b
[8] [9] [15] [16] [17] [24] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [45] [46] [47] [48] [49] [60] [61] GitHub - gemini-cli-extensions/conductor: Conductor is a Gemini CLI extension that allows you to specify, plan, and implement software features.
https://github.com/gemini-cli-extensions/conductor
[18] SPECS.md
file://file-7kvYE7Zap2vHxEcoAx47cY
[19] [20] [56] Gemini CLI + Adaptive: automatic model routing for faster, higher-quality Gemini workflows : r/GeminiAI
https://www.reddit.com/r/GeminiAI/comments/1nxpt97/gemini_cli_adaptive_automatic_model_routing_for/
[21] [22] From Coder to Conductor: Orchestrating Agents with Gemini & Jules | by Roy Reshef | Israeli Tech Radar | Nov, 2025 | Medium
https://medium.com/israeli-tech-radar/from-coder-to-conductor-orchestrating-agents-with-gemini-jules-2a2fd11589a7
[26]  Real-World Agent Examples with Gemini 3 - Google Developers Blog
https://developers.googleblog.com/real-world-agent-examples-with-gemini-3/
[28] [50] [51] [52] [55] [58] [62] [64] Welcome to Gemini CLI documentation | Gemini CLI
http://geminicli.com/docs/
[53] [54]  Introducing the Jules extension for Gemini CLI - Google Developers Blog
https://developers.googleblog.com/en/introducing-the-jules-extension-for-gemini-cli/
[59] [63] Gemini CLI settings (`/settings` command) | Gemini CLI
http://geminicli.com/docs/cli/settings/
